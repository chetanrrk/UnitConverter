# Copyright (C) Lightbend Inc. <https://www.lightbend.com>

play.modules {
  enabled += "play.filters.csrf.CSRFModule"
  enabled += "play.filters.cors.CORSModule"
  enabled += "play.filters.csp.CSPModule"
  enabled += "play.filters.headers.SecurityHeadersModule"
  enabled += "play.filters.hosts.AllowedHostsModule"
  enabled += "play.filters.gzip.GzipFilterModule"
  enabled += "play.filters.https.RedirectHttpsModule"
}

play.filters {

  # Default list of enabled filters, configured by play.api.http.EnabledFilters
  enabled += play.filters.csrf.CSRFFilter
  enabled += play.filters.headers.SecurityHeadersFilter
  enabled += play.filters.hosts.AllowedHostsFilter

  # CSRF config
  csrf {

    # Token configuration
    token {
      # The token name
      name = "csrfToken"

      # Whether tokens should be signed or not
      sign = true
    }

    # Cookie configuration
    cookie {
      # If non null, the CSRF token will be placed in a cookie with this name
      name = null

      # Whether the cookie should be set to secure
      secure = ${play.http.session.secure}

      # Whether the cookie should have the HTTP only flag set
      httpOnly = false

      # The value of the SameSite attribute of the cookie. Set to null for no SameSite attribute.
      # Possible values are "lax" and "strict". If misconfigured it's set to null.
      sameSite = ${play.http.session.sameSite}
    }

    # How much of the body should be buffered when looking for the token in the request body
    body.bufferSize = ${play.http.parser.maxMemoryBuffer}

    # Bypass the CSRF check if this origin is trusted by the CORS filter
    bypassCorsTrustedOrigins = true

    # Header configuration
    header {

      # The name of the header to accept CSRF tokens from.
      name = "Csrf-Token"


      # Defines headers that must be present to perform the CSRF check. If any of these headers are present, the CSRF
      # check will be performed.
      #
      # By default, we only perform the CSRF check if there are Cookies or an Authorization header.
      # Generally, CSRF attacks use a user's browser to execute requests on the client's behalf. If the user does not
      # have an active session, there is no danger of this happening.
      #
      # Setting this to null or an empty object will protect all requests.
      protectHeaders {
        Cookie = "*"
        Authorization = "*"
      }

      # Defines headers that can be used to bypass the CSRF check if any are present. A value of "*" simply
      # checks for the presence of the header. A string value checks for a match on that string.
      bypassHeaders {}
    }

    # Method lists
    method {
      # If non empty, then requests will be checked if the method is not in this list.
      whiteList = ["GET", "HEAD", "OPTIONS"]

      # The black list is only used if the white list is empty.
      # Only check methods in this list.
      blackList = []
    }

    # Content type lists
    # If both white lists and black lists are empty, then all content types are checked.
    contentType {
      # If non empty, then requests will be checked if the content type is not in this list.
      whiteList = []

      # The black list is only used if the white list is empty.
      # Only check content types in this list.
      blackList = []
    }

    routeModifiers {
      # If non empty, then requests will be checked if the route does not have this modifier. This is how we enable the
      # nocsrf modifier, but you may choose to use a different modifier (such as "api") if you plan to check the
      # modifier in your code for other purposes.
      whiteList = ["nocsrf"]

      # If non empty, then requests will be checked if the route contains this modifier
      # The black list is used only if the white list is empty
      blackList = []
    }

    # The error handler.
    # Used by Play's built in DI support to locate and bind a request handler.  Must be one of the following:
    # - A FQCN that implements play.filters.csrf.CSRF.ErrorHandler (Scala).
    # - A FQCN that implements play.filters.csrf.CSRFErrorHandler (Java).
    # - provided, indicates that the application has bound an instance of play.filters.csrf.CSRF.ErrorHandler through some
    #   other mechanism.
    # If null, will attempt to load a class called CSRFErrorHandler in the root package, otherwise if that's
    # not found, will default to play.filters.csrf.CSRF.CSRFHttpErrorHandler, which delegates to the configured
    # HttpRequestHandler.
    errorHandler = null
  }

  # Security headers filter configuration
  headers {

    # The X-Frame-Options header. If null, the header is not set.
    frameOptions = "DENY"

    # The X-XSS-Protection header. If null, the header is not set.
    xssProtection = "1; mode=block"

    # The X-Content-Type-Options header. If null, the header is not set.
    contentTypeOptions = "nosniff"

    # The X-Permitted-Cross-Domain-Policies header. If null, the header is not set.
    permittedCrossDomainPolicies = "master-only"

    # DEPRECATED: Content Security Policy.  If null, the header is not set.
    # This config property is set to null deliberately as the CSPFilter replaces it.
    contentSecurityPolicy = null

    # The Referrer-Policy header. If null, the header is not set.
    referrerPolicy = "origin-when-cross-origin, strict-origin-when-cross-origin"

    # If true, allow an action to use .withHeaders to replace one or more of the above headers
    allowActionSpecificHeaders = false
  }

  # Content Security Policy filter configuration
  # Please see https://playframework.com/documentation/latest/CspFilter for more details.
  csp {
    # If true, the CSP output uses Content-Security-Policy-Report-Only header instead.
    reportOnly = false

    routeModifiers {
      # If non empty, then requests will be checked if the route does not have this modifier.
      whiteList = ["nocsp"]

      # If non empty, then requests will be checked if the route contains this modifier
      # The black list is used only if the white list is empty
      blackList = []
    }

    # #csp-nonce
    # Specify a nonce to be used in CSP security header
    # https://www.w3.org/TR/CSP3/#security-nonces
    #
    # Nonces are used in script and style elements to protect against XSS attacks.
    nonce {
      # Use nonce value (generated and passed in through request attribute)
      enabled = true

      # Pattern to use to replace with nonce
      pattern = "%CSP_NONCE_PATTERN%"

      # Add the nonce to "X-Content-Security-Policy-Nonce" header.  This is useful for debugging.
      header = false
    }
    # #csp-nonce

    # Specify hashes that are used internally in the content security policy.
    # The format of these hashes are as follows:
    #
    # {
    #   algorithm = sha256
    #   hash = "RpniQm4B6bHP0cNtv7w1p6pVcgpm5B/eu1DNEYyMFXc="
    #   pattern = "%CSP_MYSCRIPT_HASH%"
    # }
    #
    # and should be used inline the same way as the nonce pattern, i.e.
    #
    # script-src = "%CSP_MYSCRIPT_HASH% 'strict-dynamic' ..."
    hashes = []

    # #csp-directives
    # The directives here are set to the Google Strict CSP policy by default
    # https://csp.withgoogle.com/docs/strict-csp.html
    directives {
      # base-uri defaults to 'none' according to https://csp.withgoogle.com/docs/strict-csp.html
      # https://www.w3.org/TR/CSP3/#directive-base-uri
      base-uri = "'none'"

      # object-src defaults to 'none' according to https://csp.withgoogle.com/docs/strict-csp.html
      # https://www.w3.org/TR/CSP3/#directive-object-src
      object-src = "'none'"

      # script-src defaults according to https://csp.withgoogle.com/docs/strict-csp.html
      # https://www.w3.org/TR/CSP3/#directive-script-src
      script-src = ${play.filters.csp.nonce.pattern} "'unsafe-inline' 'unsafe-eval' 'strict-dynamic' https: http:"
    }
    # #csp-directives
  }

  # Allowed hosts filter configuration
  hosts {

    # A list of valid hosts (e.g. "example.com") or suffixes of valid hosts (e.g. ".example.com")
    # Note that ".example.com" will match example.com and any subdomain of example.com, with or without a trailing dot.
    # "." matches all domains, and "" matches an empty or nonexistent host.
    allowed = ["localhost", ".local", "127.0.0.1"]

    routeModifiers {
      # If non empty, then requests will be checked if the route does not have this modifier. This is how we enable the
      # anyhost modifier, but you may choose to use a different modifier (such as "api") if you plan to check the
      # modifier in your code for other purposes.
      whiteList = ["anyhost"]

      # If non empty, then requests will be checked if the route contains this modifier
      # The black list is used only if the white list is empty
      blackList = []
    }
  }

  # CORS filter configuration
  cors {

    # The path prefixes to filter.
    pathPrefixes = ["/"]

    # The allowed origins. If null, all origins are allowed.
    allowedOrigins = null

    # The allowed HTTP methods. If null, all methods are allowed
    allowedHttpMethods = null

    # The allowed HTTP headers. If null, all headers are allowed.
    allowedHttpHeaders = null

    # The exposed headers
    exposedHeaders = []

    # Whether to support credentials
    supportsCredentials = true

    # The maximum amount of time the CORS meta data should be cached by the client
    preflightMaxAge = 1 hour

    # Whether to serve forbidden origins as non-CORS requests
    serveForbiddenOrigins = false
  }

  # GZip filter configuration
  gzip {

    # The maximum amount of data to send to the Gzip compressor in one go. Use `0` or `2147483647` to disable the buffering.
    #
    # In general, it is recommended to turn off the buffer and prevent generation of overlong chunks at the source.
    bufferSize = 8k

    # The maximum amount of content to buffer for gzipping in order to calculate the content length before falling back
    # to chunked encoding.
    chunkedThreshold = 100k

    contentType {

        # If non empty, then a response will only be compressed if its content type is in this list.
        whiteList = []

        # The black list is only used if the white list is empty.
        # Compress all responses except the ones whose content type is in this list.
        blackList = []
    }

    # The compression level to use, integer, -1 to 9, inclusive. See java.util.zip.Deflater.
    compressionLevel = -1

    # The byte threshold for the response body size which controls if a response should be gzipped (e.g. 1k).
    # If the body size cannot be determined, then it is assumed the response is over the threshold.
    # Set to 0 if you want to compress all responses, no matter how large the response body size is.
    threshold = 0
  }

  # Configuration for redirection to HTTPS and Strict-Transport-Security
  https {

    # A boolean defining whether the redirect to HTTPS is enabled.
    # A value of null means enabled only in Prod mode, but disabled in Dev/Test.
    redirectEnabled = null

    # The Strict-Transport-Security header is used to signal to browsers to always use https.
    # This header is added whenever a request is secure and redirectEnabled is true.
    # Set to null to disable the header.
    strictTransportSecurity = "max-age=31536000; includeSubDomains"

    # Configures the redirect status code used if the request is not secure.
    # By default, uses HTTP status code 308, which is a permanent redirect that does
    # not change the HTTP method according to [RFC 7238](https://tools.ietf.org/html/rfc7538).
    redirectStatusCode = 308

    # A boolean defining whether to only redirect if a x-forwarded-proto header is set to http.
    # This is a defacto standard that will be used by various proxys or load balancers to determine
    # if a redirect should happen.
    # [X-Forwarded-Proto](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-Proto)
    xForwardedProtoEnabled = false

    # No redirect happens if path is in this list.
    # Query params don't matter, meaning if the list contains "/foo" the request "/foo?abc=xyz" will be excluded too.
    # Paths in this list are encoded, if you want to exclude "/foöbär" you have to add "/fo%C3%B6b%C3%A4r" to the list.
    excludePaths = []

    # The HTTPS port to use in the Redirect's Location URL.
    # e.g. port = 9443 results in https://playframework.com:9443/some/url
    port = null
    port = ${?play.server.https.port} # default to same HTTPS port as play server
  }
}
# Copyright (C) Lightbend Inc. <https://www.lightbend.com>

play {
  modules {
    enabled += "play.inject.BuiltInModule"
    enabled += "play.core.FileMimeTypesModule"
    enabled += "play.core.ObjectMapperModule"
    enabled += "play.routing.RoutingDslModule"
  }
}

akka.serialization.jackson.play {
  # The "play" ObjectMapper uses the default Jackson Modules configured in akka.serialization.jackson.jackson-modules
  # but also must use "play.utils.JacksonJsonNodeModule".
  # Note: the syntax ${reference.to.an.array} ["another-value"] is lightbend config for concatenating arrays.
  jackson-modules = ${akka.serialization.jackson.jackson-modules} ["play.utils.JacksonJsonNodeModule"]
}
####################################
# Akka Actor Reference Config File #
####################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

# Akka version, checked against the runtime version of Akka. Loaded from generated conf file.
include "version"

akka {
  # Home directory of Akka, modules in the deploy directory will be loaded
  home = ""

  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.Logging$DefaultLogger"]

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream. It can perform
  # fine grained filtering based on the log source. The default
  # implementation filters on the `loglevel`.
  # FQCN of the LoggingFilter. The Class of the FQCN must implement
  # akka.event.LoggingFilter and have a public constructor with
  # (akka.actor.ActorSystem.Settings, akka.event.EventStream) parameters.
  logging-filter = "akka.event.DefaultLoggingFilter"

  # Specifies the default loggers dispatcher
  loggers-dispatcher = "akka.actor.default-dispatcher"

  # Loggers are created and registered synchronously during ActorSystem
  # start-up, and since they are actors, this timeout is used to bound the
  # waiting time
  logger-startup-timeout = 5s

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "INFO"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "WARNING"

  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off

  # Log at info level when messages are sent to dead letters, or published to
  # eventStream as `DeadLetter`, `Dropped` or `UnhandledMessage`.
  # Possible values:
  # on: all dead letters are logged
  # off: no logging of dead letters
  # n: positive integer, number of dead letters that will be logged
  log-dead-letters = 10

  # Possibility to turn off logging of dead letters while the actor system
  # is shutting down. Logging is only done when enabled by 'log-dead-letters'
  # setting.
  log-dead-letters-during-shutdown = off

  # When log-dead-letters is enabled, this will re-enable the logging after configured duration.
  # infinite: suspend the logging forever;
  # or a duration (eg: 5 minutes), after which the logging will be re-enabled.
  log-dead-letters-suspend-duration = 5 minutes

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Library extensions are regular extensions that are loaded at startup and are
  # available for third party library authors to enable auto-loading of extensions when
  # present on the classpath. This is done by appending entries:
  # 'library-extensions += "Extension"' in the library `reference.conf`.
  #
  # Should not be set by end user applications in 'application.conf', use the extensions property for that
  #
  library-extensions = ${?akka.library-extensions} ["akka.serialization.SerializationExtension$"]

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Should be on the format: 'extensions = ["foo", "bar"]' etc.
  # See the Akka Documentation for more info about Extensions
  extensions = []

  # Toggles whether threads created by this ActorSystem should be daemons or not
  daemonic = off

  # JVM shutdown, System.exit(-1), in case of a fatal error,
  # such as OutOfMemoryError
  jvm-exit-on-fatal-error = on

  # Akka installs JVM shutdown hooks by default, e.g. in CoordinatedShutdown and Artery. This property will
  # not disable user-provided hooks registered using `CoordinatedShutdown#addCancellableJvmShutdownHook`.
  # This property is related to `akka.coordinated-shutdown.run-by-jvm-shutdown-hook` below.
  # This property makes it possible to disable all such hooks if the application itself
  # or a higher level framework such as Play prefers to install the JVM shutdown hook and
  # terminate the ActorSystem itself, with or without using CoordinatedShutdown.
  jvm-shutdown-hooks = on

  # Version must be the same across all modules and if they are different the startup
  # will fail. It's possible but not recommended, to disable this check, and only log a warning,
  # by setting this property to `off`.
  fail-mixed-versions = on

  # Some modules (remoting only right now) can emit custom events to the Java Flight Recorder if running
  # on JDK 11 or later. If you for some reason do not want that, it can be disabled and switched to no-ops
  # with this toggle.
  java-flight-recorder {
    enabled = true
  }

  actor {

    # Either one of "local", "remote" or "cluster" or the
    # FQCN of the ActorRefProvider to be used; the below is the built-in default,
    # note that "remote" and "cluster" requires the akka-remote and akka-cluster
    # artifacts to be on the classpath.
    provider = "local"

    # The guardian "/user" will use this class to obtain its supervisorStrategy.
    # It needs to be a subclass of akka.actor.SupervisorStrategyConfigurator.
    # In addition to the default there is akka.actor.StoppingSupervisorStrategy.
    guardian-supervisor-strategy = "akka.actor.DefaultSupervisorStrategy"

    # Timeout for Extension creation and a few other potentially blocking
    # initialization tasks.
    creation-timeout = 20s

    # Serializes and deserializes (non-primitive) messages to ensure immutability,
    # this is only intended for testing.
    serialize-messages = off

    # Serializes and deserializes creators (in Props) to ensure that they can be
    # sent over the network, this is only intended for testing. Purely local deployments
    # as marked with deploy.scope == LocalScope are exempt from verification.
    serialize-creators = off

    # If serialize-messages or serialize-creators are enabled classes that starts with
    # a prefix listed here are not verified.
    no-serialization-verification-needed-class-prefix = ["akka."]

    # Timeout for send operations to top-level actors which are in the process
    # of being started. This is only relevant if using a bounded mailbox or the
    # CallingThreadDispatcher for a top-level actor.
    unstarted-push-timeout = 10s

    # TypedActor deprecated since 2.6.0.
    typed {
      # Default timeout for the deprecated TypedActor (not the new actor APIs in 2.6)
      # methods with non-void return type.
      timeout = 5s
    }

    # Mapping between ´deployment.router' short names to fully qualified class names
    router.type-mapping {
      from-code = "akka.routing.NoRouter"
      round-robin-pool = "akka.routing.RoundRobinPool"
      round-robin-group = "akka.routing.RoundRobinGroup"
      random-pool = "akka.routing.RandomPool"
      random-group = "akka.routing.RandomGroup"
      balancing-pool = "akka.routing.BalancingPool"
      smallest-mailbox-pool = "akka.routing.SmallestMailboxPool"
      broadcast-pool = "akka.routing.BroadcastPool"
      broadcast-group = "akka.routing.BroadcastGroup"
      scatter-gather-pool = "akka.routing.ScatterGatherFirstCompletedPool"
      scatter-gather-group = "akka.routing.ScatterGatherFirstCompletedGroup"
      tail-chopping-pool = "akka.routing.TailChoppingPool"
      tail-chopping-group = "akka.routing.TailChoppingGroup"
      consistent-hashing-pool = "akka.routing.ConsistentHashingPool"
      consistent-hashing-group = "akka.routing.ConsistentHashingGroup"
    }

    deployment {

      # deployment id pattern - on the format: /parent/child etc.
      default {

        # The id of the dispatcher to use for this actor.
        # If undefined or empty the dispatcher specified in code
        # (Props.withDispatcher) is used, or default-dispatcher if not
        # specified at all.
        dispatcher = ""

        # The id of the mailbox to use for this actor.
        # If undefined or empty the default mailbox of the configured dispatcher
        # is used or if there is no mailbox configuration the mailbox specified
        # in code (Props.withMailbox) is used.
        # If there is a mailbox defined in the configured dispatcher then that
        # overrides this setting.
        mailbox = ""

        # routing (load-balance) scheme to use
        # - available: "from-code", "round-robin", "random", "smallest-mailbox",
        #              "scatter-gather", "broadcast"
        # - or:        Fully qualified class name of the router class.
        #              The class must extend akka.routing.CustomRouterConfig and
        #              have a public constructor with com.typesafe.config.Config
        #              and optional akka.actor.DynamicAccess parameter.
        # - default is "from-code";
        # Whether or not an actor is transformed to a Router is decided in code
        # only (Props.withRouter). The type of router can be overridden in the
        # configuration; specifying "from-code" means that the values specified
        # in the code shall be used.
        # In case of routing, the actors to be routed to can be specified
        # in several ways:
        # - nr-of-instances: will create that many children
        # - routees.paths: will route messages to these paths using ActorSelection,
        #   i.e. will not create children
        # - resizer: dynamically resizable number of routees as specified in
        #   resizer below
        router = "from-code"

        # number of children to create in case of a router;
        # this setting is ignored if routees.paths is given
        nr-of-instances = 1

        # within is the timeout used for routers containing future calls
        within = 5 seconds

        # number of virtual nodes per node for consistent-hashing router
        virtual-nodes-factor = 10

        tail-chopping-router {
          # interval is duration between sending message to next routee
          interval = 10 milliseconds
        }

        routees {
          # Alternatively to giving nr-of-instances you can specify the full
          # paths of those actors which should be routed to. This setting takes
          # precedence over nr-of-instances
          paths = []
        }

        # To use a dedicated dispatcher for the routees of the pool you can
        # define the dispatcher configuration inline with the property name
        # 'pool-dispatcher' in the deployment section of the router.
        # For example:
        # pool-dispatcher {
        #   fork-join-executor.parallelism-min = 5
        #   fork-join-executor.parallelism-max = 5
        # }

        # Routers with dynamically resizable number of routees; this feature is
        # enabled by including (parts of) this section in the deployment
        resizer {

          enabled = off

          # The fewest number of routees the router should ever have.
          lower-bound = 1

          # The most number of routees the router should ever have.
          # Must be greater than or equal to lower-bound.
          upper-bound = 10

          # Threshold used to evaluate if a routee is considered to be busy
          # (under pressure). Implementation depends on this value (default is 1).
          # 0:   number of routees currently processing a message.
          # 1:   number of routees currently processing a message has
          #      some messages in mailbox.
          # > 1: number of routees with at least the configured pressure-threshold
          #      messages in their mailbox. Note that estimating mailbox size of
          #      default UnboundedMailbox is O(N) operation.
          pressure-threshold = 1

          # Percentage to increase capacity whenever all routees are busy.
          # For example, 0.2 would increase 20% (rounded up), i.e. if current
          # capacity is 6 it will request an increase of 2 more routees.
          rampup-rate = 0.2

          # Minimum fraction of busy routees before backing off.
          # For example, if this is 0.3, then we'll remove some routees only when
          # less than 30% of routees are busy, i.e. if current capacity is 10 and
          # 3 are busy then the capacity is unchanged, but if 2 or less are busy
          # the capacity is decreased.
          # Use 0.0 or negative to avoid removal of routees.
          backoff-threshold = 0.3

          # Fraction of routees to be removed when the resizer reaches the
          # backoffThreshold.
          # For example, 0.1 would decrease 10% (rounded up), i.e. if current
          # capacity is 9 it will request an decrease of 1 routee.
          backoff-rate = 0.1

          # Number of messages between resize operation.
          # Use 1 to resize before each message.
          messages-per-resize = 10
        }

        # Routers with dynamically resizable number of routees based on
        # performance metrics.
        # This feature is enabled by including (parts of) this section in
        # the deployment, cannot be enabled together with default resizer.
        optimal-size-exploring-resizer {

          enabled = off

          # The fewest number of routees the router should ever have.
          lower-bound = 1

          # The most number of routees the router should ever have.
          # Must be greater than or equal to lower-bound.
          upper-bound = 10

          # probability of doing a ramping down when all routees are busy
          # during exploration.
          chance-of-ramping-down-when-full = 0.2

          # Interval between each resize attempt
          action-interval = 5s

          # If the routees have not been fully utilized (i.e. all routees busy)
          # for such length, the resizer will downsize the pool.
          downsize-after-underutilized-for = 72h

          # Duration exploration, the ratio between the largest step size and
          # current pool size. E.g. if the current pool size is 50, and the
          # explore-step-size is 0.1, the maximum pool size change during
          # exploration will be +- 5
          explore-step-size = 0.1

          # Probability of doing an exploration v.s. optimization.
          chance-of-exploration = 0.4

          # When downsizing after a long streak of underutilization, the resizer
          # will downsize the pool to the highest utiliziation multiplied by a
          # a downsize ratio. This downsize ratio determines the new pools size
          # in comparison to the highest utilization.
          # E.g. if the highest utilization is 10, and the down size ratio
          # is 0.8, the pool will be downsized to 8
          downsize-ratio = 0.8

          # When optimizing, the resizer only considers the sizes adjacent to the
          # current size. This number indicates how many adjacent sizes to consider.
          optimization-range = 16

          # The weight of the latest metric over old metrics when collecting
          # performance metrics.
          # E.g. if the last processing speed is 10 millis per message at pool
          # size 5, and if the new processing speed collected is 6 millis per
          # message at pool size 5. Given a weight of 0.3, the metrics
          # representing pool size 5 will be 6 * 0.3 + 10 * 0.7, i.e. 8.8 millis
          # Obviously, this number should be between 0 and 1.
          weight-of-latest-metric = 0.5
        }
      }

      "/IO-DNS/inet-address" {
        mailbox = "unbounded"
        router = "consistent-hashing-pool"
        nr-of-instances = 4
      }

      "/IO-DNS/inet-address/*" {
        dispatcher = "akka.actor.default-blocking-io-dispatcher"
      }

      "/IO-DNS/async-dns" {
        mailbox = "unbounded"
        router = "round-robin-pool"
        nr-of-instances = 1
      }
    }

    default-dispatcher {
      # Must be one of the following
      # Dispatcher, PinnedDispatcher, or a FQCN to a class inheriting
      # MessageDispatcherConfigurator with a public constructor with
      # both com.typesafe.config.Config parameter and
      # akka.dispatch.DispatcherPrerequisites parameters.
      # PinnedDispatcher must be used together with executor=thread-pool-executor.
      type = "Dispatcher"

      # Which kind of ExecutorService to use for this dispatcher
      # Valid options:
      #  - "default-executor" requires a "default-executor" section
      #  - "fork-join-executor" requires a "fork-join-executor" section
      #  - "thread-pool-executor" requires a "thread-pool-executor" section
      #  - "affinity-pool-executor" requires an "affinity-pool-executor" section
      #  - A FQCN of a class extending ExecutorServiceConfigurator
      executor = "default-executor"

      # This will be used if you have set "executor = "default-executor"".
      # If an ActorSystem is created with a given ExecutionContext, this
      # ExecutionContext will be used as the default executor for all
      # dispatchers in the ActorSystem configured with
      # executor = "default-executor". Note that "default-executor"
      # is the default value for executor, and therefore used if not
      # specified otherwise. If no ExecutionContext is given,
      # the executor configured in "fallback" will be used.
      default-executor {
        fallback = "fork-join-executor"
      }

      # This will be used if you have set "executor = "affinity-pool-executor""
      # Underlying thread pool implementation is akka.dispatch.affinity.AffinityPool.
      # This executor is classified as "ApiMayChange".
      affinity-pool-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 4

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 0.8

        # Max number of threads to cap factor-based parallelism number to.
        parallelism-max = 64

        # Each worker in the pool uses a separate bounded MPSC queue. This value
        # indicates the upper bound of the queue. Whenever an attempt to enqueue
        # a task is made and the queue does not have capacity to accommodate
        # the task, the rejection handler created by the rejection handler specified
        # in "rejection-handler" is invoked.
        task-queue-size = 512

        # FQCN of the Rejection handler used in the pool.
        # Must have an empty public constructor and must
        # implement akka.actor.affinity.RejectionHandlerFactory.
        rejection-handler = "akka.dispatch.affinity.ThrowOnOverflowRejectionHandler"

        # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
        # The tradeoff is that to have low latency more CPU time must be used to be
        # able to react quickly on incoming messages or send as fast as possible after
        # backoff backpressure.
        # Level 1 strongly prefer low CPU consumption over low latency.
        # Level 10 strongly prefer low latency over low CPU consumption.
        idle-cpu-level = 5

        # FQCN of the akka.dispatch.affinity.QueueSelectorFactory.
        # The Class of the FQCN must have a public constructor with a
        # (com.typesafe.config.Config) parameter.
        # A QueueSelectorFactory create instances of akka.dispatch.affinity.QueueSelector,
        # that is responsible for determining which task queue a Runnable should be enqueued in.
        queue-selector = "akka.dispatch.affinity.FairDistributionHashCache"

        # When using the "akka.dispatch.affinity.FairDistributionHashCache" queue selector
        # internally the AffinityPool uses two methods to determine which task
        # queue to allocate a Runnable to:
        # - map based - maintains a round robin counter and a map of Runnable
        # hashcodes to queues that they have been associated with. This ensures
        # maximum fairness in terms of work distribution, meaning that each worker
        # will get approximately equal amount of mailboxes to execute. This is suitable
        # in cases where we have a small number of actors that will be scheduled on
        # the pool and we want to ensure the maximum possible utilization of the
        # available threads.
        # - hash based - the task - queue in which the runnable should go is determined
        # by using an uniformly distributed int to int hash function which uses the
        # hash code of the Runnable as an input. This is preferred in situations where we
        # have enough number of distinct actors to ensure statistically uniform
        # distribution of work across threads or we are ready to sacrifice the
        # former for the added benefit of avoiding map look-ups.
        fair-work-distribution {
          # The value serves as a threshold which determines the point at which the
          # pool switches from the first to the second work distribution schemes.
          # For example, if the value is set to 128, the pool can observe up to
          # 128 unique actors and schedule their mailboxes using the map based
          # approach. Once this number is reached the pool switches to hash based
          # task distribution mode. If the value is set to 0, the map based
          # work distribution approach is disabled and only the hash based is
          # used irrespective of the number of unique actors. Valid range is
          # 0 to 2048 (inclusive)
          threshold = 128
        }
      }

      # This will be used if you have set "executor = "fork-join-executor""
      # Underlying thread pool implementation is java.util.concurrent.ForkJoinPool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 8

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 1.0

        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 64

        # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
        # like peeking mode which "pop".
        task-peeking-mode = "FIFO"
      }

      # This will be used if you have set "executor = "thread-pool-executor""
      # Underlying thread pool implementation is java.util.concurrent.ThreadPoolExecutor
      thread-pool-executor {
        # Keep alive time for threads
        keep-alive-time = 60s

        # Define a fixed thread pool size with this property. The corePoolSize
        # and the maximumPoolSize of the ThreadPoolExecutor will be set to this
        # value, if it is defined. Then the other pool-size properties will not
        # be used.
        #
        # Valid values are: `off` or a positive integer.
        fixed-pool-size = off

        # Min number of threads to cap factor-based corePoolSize number to
        core-pool-size-min = 8

        # The core-pool-size-factor is used to determine corePoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor).
        # Resulting size is then bounded by the core-pool-size-min and
        # core-pool-size-max values.
        core-pool-size-factor = 3.0

        # Max number of threads to cap factor-based corePoolSize number to
        core-pool-size-max = 64

        # Minimum number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-min = 8

        # The max-pool-size-factor is used to determine maximumPoolSize of the
        # ThreadPoolExecutor using the following formula:
        # ceil(available processors * factor)
        # The maximumPoolSize will not be less than corePoolSize.
        # It is only used if using a bounded task queue.
        max-pool-size-factor  = 3.0

        # Max number of threads to cap factor-based maximumPoolSize number to
        max-pool-size-max = 64

        # Specifies the bounded capacity of the task queue (< 1 == unbounded)
        task-queue-size = -1

        # Specifies which type of task queue will be used, can be "array" or
        # "linked" (default)
        task-queue-type = "linked"

        # Allow core threads to time out
        allow-core-timeout = on
      }

      # How long time the dispatcher will wait for new actors until it shuts down
      shutdown-timeout = 1s

      # Throughput defines the number of messages that are processed in a batch
      # before the thread is returned to the pool. Set to 1 for as fair as possible.
      throughput = 5

      # Throughput deadline for Dispatcher, set to 0 or negative for no deadline
      throughput-deadline-time = 0ms

      # For BalancingDispatcher: If the balancing dispatcher should attempt to
      # schedule idle actors using the same dispatcher when a message comes in,
      # and the dispatchers ExecutorService is not fully busy already.
      attempt-teamwork = on

      # If this dispatcher requires a specific type of mailbox, specify the
      # fully-qualified class name here; the actually created mailbox will
      # be a subtype of this type. The empty string signifies no requirement.
      mailbox-requirement = ""
    }

    # Default separate internal dispatcher to run Akka internal tasks and actors on
    # protecting them against starvation because of accidental blocking in user actors (which run on the
    # default dispatcher)
    internal-dispatcher {
      type = "Dispatcher"
      executor = "fork-join-executor"
      throughput = 5
      fork-join-executor {
        parallelism-min = 4
        parallelism-factor = 1.0
        parallelism-max = 64
      }
    }

    default-blocking-io-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      throughput = 1

      thread-pool-executor {
        fixed-pool-size = 16
      }
    }

    default-mailbox {
      # FQCN of the MailboxType. The Class of the FQCN must have a public
      # constructor with
      # (akka.actor.ActorSystem.Settings, com.typesafe.config.Config) parameters.
      mailbox-type = "akka.dispatch.UnboundedMailbox"

      # If the mailbox is bounded then it uses this setting to determine its
      # capacity. The provided value must be positive.
      # NOTICE:
      # Up to version 2.1 the mailbox type was determined based on this setting;
      # this is no longer the case, the type must explicitly be a bounded mailbox.
      mailbox-capacity = 1000

      # If the mailbox is bounded then this is the timeout for enqueueing
      # in case the mailbox is full. Negative values signify infinite
      # timeout, which should be avoided as it bears the risk of dead-lock.
      mailbox-push-timeout-time = 10s

      # For Actor with Stash: The default capacity of the stash.
      # If negative (or zero) then an unbounded stash is used (default)
      # If positive then a bounded stash is used and the capacity is set using
      # the property
      stash-capacity = -1
    }

    mailbox {
      # Mapping between message queue semantics and mailbox configurations.
      # Used by akka.dispatch.RequiresMessageQueue[T] to enforce different
      # mailbox types on actors.
      # If your Actor implements RequiresMessageQueue[T], then when you create
      # an instance of that actor its mailbox type will be decided by looking
      # up a mailbox configuration via T in this mapping
      requirements {
        "akka.dispatch.UnboundedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-queue-based
        "akka.dispatch.BoundedMessageQueueSemantics" =
          akka.actor.mailbox.bounded-queue-based
        "akka.dispatch.DequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-deque-based
        "akka.dispatch.UnboundedDequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-deque-based
        "akka.dispatch.BoundedDequeBasedMessageQueueSemantics" =
          akka.actor.mailbox.bounded-deque-based
        "akka.dispatch.MultipleConsumerSemantics" =
          akka.actor.mailbox.unbounded-queue-based
        "akka.dispatch.ControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-control-aware-queue-based
        "akka.dispatch.UnboundedControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.unbounded-control-aware-queue-based
        "akka.dispatch.BoundedControlAwareMessageQueueSemantics" =
          akka.actor.mailbox.bounded-control-aware-queue-based
        "akka.event.LoggerMessageQueueSemantics" =
          akka.actor.mailbox.logger-queue
      }

      unbounded-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedMailbox"
      }

      bounded-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedMailbox"
      }

      unbounded-deque-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedDequeBasedMailbox"
      }

      bounded-deque-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedDequeBasedMailbox"
      }

      unbounded-control-aware-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"
      }

      bounded-control-aware-queue-based {
        # FQCN of the MailboxType, The Class of the FQCN must have a public
        # constructor with (akka.actor.ActorSystem.Settings,
        # com.typesafe.config.Config) parameters.
        mailbox-type = "akka.dispatch.BoundedControlAwareMailbox"
      }

      # The LoggerMailbox will drain all messages in the mailbox
      # when the system is shutdown and deliver them to the StandardOutLogger.
      # Do not change this unless you know what you are doing.
      logger-queue {
        mailbox-type = "akka.event.LoggerMailboxType"
      }
    }

    debug {
      # enable function of Actor.loggable(), which is to log any received message
      # at DEBUG level, see the “Testing Actor Systems” section of the Akka
      # Documentation at http://akka.io/docs
      receive = off

      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill etc.)
      autoreceive = off

      # enable DEBUG logging of actor lifecycle changes
      lifecycle = off

      # enable DEBUG logging of all LoggingFSMs for events, transitions and timers
      fsm = off

      # enable DEBUG logging of subscription changes on the eventStream
      event-stream = off

      # enable DEBUG logging of unhandled messages
      unhandled = off

      # enable WARN logging of misconfigured routers
      router-misconfiguration = off
    }

    # SECURITY BEST-PRACTICE is to disable java serialization for its multiple
    # known attack surfaces.
    #
    # This setting is a short-cut to
    # - using DisabledJavaSerializer instead of JavaSerializer
    #
    # Completely disable the use of `akka.serialization.JavaSerialization` by the
    # Akka Serialization extension, instead DisabledJavaSerializer will
    # be inserted which will fail explicitly if attempts to use java serialization are made.
    #
    # The log messages emitted by such serializer SHOULD be treated as potential
    # attacks which the serializer prevented, as they MAY indicate an external operator
    # attempting to send malicious messages intending to use java serialization as attack vector.
    # The attempts are logged with the SECURITY marker.
    #
    # Please note that this option does not stop you from manually invoking java serialization
    #
    allow-java-serialization = off

    # Log warnings when the Java serialization is used to serialize messages.
    # Java serialization is not very performant and should not be used in production
    # environments unless you don't care about performance and security. In that case
    # you can turn this off.
    warn-about-java-serializer-usage = on

    # To be used with the above warn-about-java-serializer-usage
    # When warn-about-java-serializer-usage = on, and this warn-on-no-serialization-verification = off,
    # warnings are suppressed for classes extending NoSerializationVerificationNeeded
    # to reduce noise.
    warn-on-no-serialization-verification = on

    # Entries for pluggable serializers and their bindings.
    serializers {
      java = "akka.serialization.JavaSerializer"
      bytes = "akka.serialization.ByteArraySerializer"
      primitive-long = "akka.serialization.LongSerializer"
      primitive-int = "akka.serialization.IntSerializer"
      primitive-string = "akka.serialization.StringSerializer"
      primitive-bytestring = "akka.serialization.ByteStringSerializer"
      primitive-boolean = "akka.serialization.BooleanSerializer"
    }

    # Class to Serializer binding. You only need to specify the name of an
    # interface or abstract base class of the messages. In case of ambiguity it
    # is using the most specific configured class, or giving a warning and
    # choosing the “first” one.
    #
    # To disable one of the default serializers, assign its class to "none", like
    # "java.io.Serializable" = none
    serialization-bindings {
      "[B" = bytes
      "java.io.Serializable" = java

      "java.lang.String" = primitive-string
      "akka.util.ByteString$ByteString1C" = primitive-bytestring
      "akka.util.ByteString$ByteString1" = primitive-bytestring
      "akka.util.ByteString$ByteStrings" = primitive-bytestring
      "java.lang.Long" = primitive-long
      "scala.Long" = primitive-long
      "java.lang.Integer" = primitive-int
      "scala.Int" = primitive-int
      "java.lang.Boolean" = primitive-boolean
      "scala.Boolean" = primitive-boolean
    }

    # Configuration namespace of serialization identifiers.
    # Each serializer implementation must have an entry in the following format:
    # `akka.actor.serialization-identifiers."FQCN" = ID`
    # where `FQCN` is fully qualified class name of the serializer implementation
    # and `ID` is globally unique serializer identifier number.
    # Identifier values from 0 to 40 are reserved for Akka internal usage.
    serialization-identifiers {
      "akka.serialization.JavaSerializer" = 1
      "akka.serialization.ByteArraySerializer" = 4

      primitive-long = 18
      primitive-int = 19
      primitive-string = 20
      primitive-bytestring = 21
      primitive-boolean = 35
    }

  }

  serialization.protobuf {
    # deprecated, use `allowed-classes` instead
    whitelist-class = [
      "com.google.protobuf.GeneratedMessage",
      "com.google.protobuf.GeneratedMessageV3",
      "scalapb.GeneratedMessageCompanion",
      "akka.protobuf.GeneratedMessage",
      "akka.protobufv3.internal.GeneratedMessageV3"
    ]

    # Additional classes that are allowed even if they are not defined in `serialization-bindings`.
    # It can be exact class name or name of super class or interfaces (one level).
    # This is useful when a class is not used for serialization any more and therefore removed
    # from `serialization-bindings`, but should still be possible to deserialize.
    allowed-classes = ${akka.serialization.protobuf.whitelist-class}

  }

  # Used to set the behavior of the scheduler.
  # Changing the default values may change the system behavior drastically so make
  # sure you know what you're doing! See the Scheduler section of the Akka
  # Documentation for more details.
  scheduler {
    # The LightArrayRevolverScheduler is used as the default scheduler in the
    # system. It does not execute the scheduled tasks on exact time, but on every
    # tick, it will run everything that is (over)due. You can increase or decrease
    # the accuracy of the execution timing by specifying smaller or larger tick
    # duration. If you are scheduling a lot of tasks you should consider increasing
    # the ticks per wheel.
    # Note that it might take up to 1 tick to stop the Timer, so setting the
    # tick-duration to a high value will make shutting down the actor system
    # take longer.
    tick-duration = 10ms

    # The timer uses a circular wheel of buckets to store the timer tasks.
    # This should be set such that the majority of scheduled timeouts (for high
    # scheduling frequency) will be shorter than one rotation of the wheel
    # (ticks-per-wheel * ticks-duration)
    # THIS MUST BE A POWER OF TWO!
    ticks-per-wheel = 512

    # This setting selects the timer implementation which shall be loaded at
    # system start-up.
    # The class given here must implement the akka.actor.Scheduler interface
    # and offer a public constructor which takes three arguments:
    #  1) com.typesafe.config.Config
    #  2) akka.event.LoggingAdapter
    #  3) java.util.concurrent.ThreadFactory
    implementation = akka.actor.LightArrayRevolverScheduler

    # When shutting down the scheduler, there will typically be a thread which
    # needs to be stopped, and this timeout determines how long to wait for
    # that to happen. In case of timeout the shutdown of the actor system will
    # proceed without running possibly still enqueued tasks.
    shutdown-timeout = 5s
  }

  io {

    # By default the select loops run on dedicated threads, hence using a
    # PinnedDispatcher
    pinned-dispatcher {
      type = "PinnedDispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.allow-core-timeout = off
    }

    tcp {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this TCP module; there is
      # no intrinsic general limit, this setting is meant to enable DoS
      # protection by limiting the number of concurrently connected clients.
      # Also note that this is a "soft" limit; in certain cases the implementation
      # will accept a few connections more or a few less than the number configured
      # here. Must be an integer > 0 or "unlimited".
      max-channels = 256000

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of connection that are accepted in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      batch-accept-limit = 10

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # The duration a connection actor waits for a `Register` message from
      # its commander before aborting the connection.
      register-timeout = 5s

      # The maximum number of bytes delivered by a `Received` message. Before
      # more data is read from the network the connection actor will try to
      # do other work.
      # The purpose of this setting is to impose a smaller limit than the
      # configured receive buffer size. When using value 'unlimited' it will
      # try to read all from the receive buffer.
      max-received-message-size = unlimited

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # on which file IO tasks are scheduled
      file-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

      # The maximum number of bytes (or "unlimited") to transfer in one batch
      # when using `WriteFile` command which uses `FileChannel.transferTo` to
      # pipe files to a TCP socket. On some OS like Linux `FileChannel.transferTo`
      # may block for a long time when network IO is faster than file IO.
      # Decreasing the value may improve fairness while increasing may improve
      # throughput.
      file-io-transferTo-limit = 512 KiB

      # The number of times to retry the `finishConnect` call after being notified about
      # OP_CONNECT. Retries are needed if the OP_CONNECT notification doesn't imply that
      # `finishConnect` will succeed, which is the case on Android.
      finish-connect-retries = 5

      # On Windows connection aborts are not reliably detected unless an OP_READ is
      # registered on the selector _after_ the connection has been reset. This
      # workaround enables an OP_CONNECT which forces the abort to be visible on Windows.
      # Enabling this setting on other platforms than Windows will cause various failures
      # and undefined behavior.
      # Possible values of this key are on, off and auto where auto will enable the
      # workaround if Windows is detected automatically.
      windows-connection-abort-workaround-enabled = off
    }

    udp {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this UDP module Generally
      # UDP does not require a large number of channels, therefore it is
      # recommended to keep this setting low.
      max-channels = 4096

      # The select loop can be used in two modes:
      # - setting "infinite" will select without a timeout, hogging a thread
      # - setting a positive timeout will do a bounded select call,
      #   enabling sharing of a single thread between multiple selectors
      #   (in this case you will have to use a different configuration for the
      #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
      # - setting it to zero means polling, i.e. calling selectNow()
      select-timeout = infinite

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of datagrams that are read in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      receive-throughput = 3

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.internal-dispatcher"
    }

    udp-connected {

      # The number of selectors to stripe the served channels over; each of
      # these will use one select loop on the selector-dispatcher.
      nr-of-selectors = 1

      # Maximum number of open channels supported by this UDP module Generally
      # UDP does not require a large number of channels, therefore it is
      # recommended to keep this setting low.
      max-channels = 4096

      # The select loop can be used in two modes:
      # - setting "infinite" will select without a timeout, hogging a thread
      # - setting a positive timeout will do a bounded select call,
      #   enabling sharing of a single thread between multiple selectors
      #   (in this case you will have to use a different configuration for the
      #   selector-dispatcher, e.g. using "type=Dispatcher" with size 1)
      # - setting it to zero means polling, i.e. calling selectNow()
      select-timeout = infinite

      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of datagrams that are read in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      receive-throughput = 3

      # The number of bytes per direct buffer in the pool used to read or write
      # network data from the kernel.
      direct-buffer-size = 128 KiB

      # The maximal number of direct buffers kept in the direct buffer pool for
      # reuse.
      direct-buffer-pool-limit = 1000

      # Enable fine grained logging of what goes on inside the implementation.
      # Be aware that this may log more than once per message sent to the actors
      # of the tcp implementation.
      trace-logging = off

      # Fully qualified config path which holds the dispatcher configuration
      # to be used for running the select() calls in the selectors
      selector-dispatcher = "akka.io.pinned-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the read/write worker actors
      worker-dispatcher = "akka.actor.internal-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # for the selector management actors
      management-dispatcher = "akka.actor.internal-dispatcher"
    }

    dns {
      # Fully qualified config path which holds the dispatcher configuration
      # for the manager and resolver router actors.
      # For actual router configuration see akka.actor.deployment./IO-DNS/*
      dispatcher = "akka.actor.internal-dispatcher"

      # Name of the subconfig at path akka.io.dns, see inet-address below
      #
      # Change to `async-dns` to use the new "native" DNS resolver,
      # which is also capable of resolving SRV records.
      resolver = "inet-address"

      # To-be-deprecated DNS resolver implementation which uses the Java InetAddress to resolve DNS records.
      # To be replaced by `akka.io.dns.async` which implements the DNS protocol natively and without blocking (which InetAddress does)
      inet-address {
        # Must implement akka.io.DnsProvider
        provider-object = "akka.io.InetAddressDnsProvider"

        # To set the time to cache name resolutions
        # Possible values:
        # default: sun.net.InetAddressCachePolicy.get() and getNegative()
        # forever: cache forever
        # never: no caching
        # n [time unit]: positive timeout with unit, for example 30s
        positive-ttl = default
        negative-ttl = default

        # How often to sweep out expired cache entries.
        # Note that this interval has nothing to do with TTLs
        cache-cleanup-interval = 120s
      }

      async-dns {
        provider-object = "akka.io.dns.internal.AsyncDnsProvider"

        # Set upper bound for caching successfully resolved dns entries
        # if the DNS record has a smaller TTL value than the setting that
        # will be used. Default is to use the record TTL with no cap.
        # Possible values:
        # forever: always use the minimum TTL from the found records
        # never: never cache
        # n [time unit] = cap the caching to this value
        positive-ttl = forever

        # Set how long the fact that a DNS record could not be found is
        # cached. If a new resolution is done while the fact is cached it will
        # be failed and not result in an actual DNS resolution. Default is
        # to never cache.
        # Possible values:
        # never: never cache
        # forever: cache a missing DNS record forever (you probably will not want to do this)
        # n [time unit] = cache for this long
        negative-ttl = never

        # Configures nameservers to query during DNS resolution.
        # Defaults to the nameservers that would be used by the JVM by default.
        # Set to a list of IPs to override the servers, e.g. [ "8.8.8.8", "8.8.4.4" ] for Google's servers
        # If multiple are defined then they are tried in order until one responds
        nameservers = default

        # The time that a request is allowed to live before being discarded
        # given no reply. The lower bound of this should always be the amount
        # of time to reasonably expect a DNS server to reply within.
        # If multiple name servers are provided then each gets this long to response before trying
        # the next one
        resolve-timeout = 5s

        # How often to sweep out expired cache entries.
        # Note that this interval has nothing to do with TTLs
        cache-cleanup-interval = 120s

        # Configures the list of search domains.
        # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on
        # other platforms, will not make any attempt to lookup the search domains). Set to a single domain, or
        # a list of domains, eg, [ "example.com", "example.net" ].
        search-domains = default

        # Any hosts that have a number of dots less than this will not be looked up directly, instead, a search on
        # the search domains will be tried first. This corresponds to the ndots option in /etc/resolv.conf, see
        # https://linux.die.net/man/5/resolver for more info.
        # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on
        # other platforms, will default to 1).
        ndots = default
      }
    }
  }


  # CoordinatedShutdown is an extension that will perform registered
  # tasks in the order that is defined by the phases. It is started
  # by calling CoordinatedShutdown(system).run(). This can be triggered
  # by different things, for example:
  # - JVM shutdown hook will by default run CoordinatedShutdown
  # - Cluster node will automatically run CoordinatedShutdown when it
  #   sees itself as Exiting
  # - A management console or other application specific command can
  #   run CoordinatedShutdown
  coordinated-shutdown {
    # The timeout that will be used for a phase if not specified with
    # 'timeout' in the phase
    default-phase-timeout = 5 s

    # Terminate the ActorSystem in the last phase actor-system-terminate.
    terminate-actor-system = on

    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = off

    # Exit status to use on System.exit(int) when 'exit-jvm' is 'on'.
    exit-code = 0

    # Run the coordinated shutdown when the JVM process exits, e.g.
    # via kill SIGTERM signal (SIGINT ctrl-c doesn't work).
    # This property is related to `akka.jvm-shutdown-hooks` above.
    run-by-jvm-shutdown-hook = on

    # Run the coordinated shutdown when ActorSystem.terminate is called.
    # Enabling this and disabling terminate-actor-system is not a supported
    # combination (will throw ConfigurationException at startup).
    run-by-actor-system-terminate = on

    # When Coordinated Shutdown is triggered an instance of `Reason` is
    # required. That value can be used to override the default settings.
    # Only 'exit-jvm', 'exit-code' and 'terminate-actor-system' may be
    # overridden depending on the reason.
    reason-overrides {
      # Overrides are applied using the `reason.getClass.getName`.
      # Overrides the `exit-code` when the `Reason` is a cluster
      # Downing or a Cluster Join Unsuccessful event
      "akka.actor.CoordinatedShutdown$ClusterDowningReason$" {
        exit-code = -1
      }
      "akka.actor.CoordinatedShutdown$ClusterJoinUnsuccessfulReason$" {
        exit-code = -1
      }
    }

    #//#coordinated-shutdown-phases
    # CoordinatedShutdown is enabled by default and will run the tasks that
    # are added to these phases by individual Akka modules and user logic.
    #
    # The phases are ordered as a DAG by defining the dependencies between the phases
    # to make sure shutdown tasks are run in the right order.
    #
    # In general user tasks belong in the first few phases, but there may be use
    # cases where you would want to hook in new phases or register tasks later in
    # the DAG.
    #
    # Each phase is defined as a named config section with the
    # following optional properties:
    # - timeout=15s: Override the default-phase-timeout for this phase.
    # - recover=off: If the phase fails the shutdown is aborted
    #                and depending phases will not be executed.
    # - enabled=off: Skip all tasks registered in this phase. DO NOT use
    #                this to disable phases unless you are absolutely sure what the
    #                consequences are. Many of the built in tasks depend on other tasks
    #                having been executed in earlier phases and may break if those are disabled.
    # depends-on=[]: Run the phase after the given phases
    phases {

      # The first pre-defined phase that applications can add tasks to.
      # Note that more phases can be added in the application's
      # configuration by overriding this phase with an additional
      # depends-on.
      before-service-unbind {
      }

      # Stop accepting new incoming connections.
      # This is where you can register tasks that makes a server stop accepting new connections. Already
      # established connections should be allowed to continue and complete if possible.
      service-unbind {
        depends-on = [before-service-unbind]
      }

      # Wait for requests that are in progress to be completed.
      # This is where you register tasks that will wait for already established connections to complete, potentially
      # also first telling them that it is time to close down.
      service-requests-done {
        depends-on = [service-unbind]
      }

      # Final shutdown of service endpoints.
      # This is where you would add tasks that forcefully kill connections that are still around.
      service-stop {
        depends-on = [service-requests-done]
      }

      # Phase for custom application tasks that are to be run
      # after service shutdown and before cluster shutdown.
      before-cluster-shutdown {
        depends-on = [service-stop]
      }

      # Graceful shutdown of the Cluster Sharding regions.
      # This phase is not meant for users to add tasks to.
      cluster-sharding-shutdown-region {
        timeout = 10 s
        depends-on = [before-cluster-shutdown]
      }

      # Emit the leave command for the node that is shutting down.
      # This phase is not meant for users to add tasks to.
      cluster-leave {
        depends-on = [cluster-sharding-shutdown-region]
      }

      # Shutdown cluster singletons
      # This is done as late as possible to allow the shard region shutdown triggered in
      # the "cluster-sharding-shutdown-region" phase to complete before the shard coordinator is shut down.
      # This phase is not meant for users to add tasks to.
      cluster-exiting {
        timeout = 10 s
        depends-on = [cluster-leave]
      }

      # Wait until exiting has been completed
      # This phase is not meant for users to add tasks to.
      cluster-exiting-done {
        depends-on = [cluster-exiting]
      }

      # Shutdown the cluster extension
      # This phase is not meant for users to add tasks to.
      cluster-shutdown {
        depends-on = [cluster-exiting-done]
      }

      # Phase for custom application tasks that are to be run
      # after cluster shutdown and before ActorSystem termination.
      before-actor-system-terminate {
        depends-on = [cluster-shutdown]
      }

      # Last phase. See terminate-actor-system and exit-jvm above.
      # Don't add phases that depends on this phase because the
      # dispatcher and scheduler of the ActorSystem have been shutdown.
      # This phase is not meant for users to add tasks to.
      actor-system-terminate {
        timeout = 10 s
        depends-on = [before-actor-system-terminate]
      }
    }
    #//#coordinated-shutdown-phases
  }

}
# Copyright (C) Lightbend Inc. <https://www.lightbend.com>

play.application.loader = "play.api.inject.guice.GuiceApplicationLoader"
#####################################
# Akka Stream Reference Config File #
#####################################

# eager creation of the system wide materializer
akka.library-extensions += "akka.stream.SystemMaterializer$"
akka {
  stream {

    # Default materializer settings
    materializer {

      # Initial size of buffers used in stream elements
      initial-input-buffer-size = 4
      # Maximum size of buffers used in stream elements
      max-input-buffer-size = 16

      # Fully qualified config path which holds the dispatcher configuration
      # or full dispatcher configuration to be used by ActorMaterializer when creating Actors.
      dispatcher = "akka.actor.default-dispatcher"

      # Fully qualified config path which holds the dispatcher configuration
      # or full dispatcher configuration to be used by stream operators that
      # perform blocking operations
      blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

      # Cleanup leaked publishers and subscribers when they are not used within a given
      # deadline
      subscription-timeout {
        # when the subscription timeout is reached one of the following strategies on
        # the "stale" publisher:
        # cancel - cancel it (via `onError` or subscribing to the publisher and
        #          `cancel()`ing the subscription right away
        # warn   - log a warning statement about the stale element (then drop the
        #          reference to it)
        # noop   - do nothing (not recommended)
        mode = cancel

        # time after which a subscriber / publisher is considered stale and eligible
        # for cancelation (see `akka.stream.subscription-timeout.mode`)
        timeout = 5s
      }

      # Enable additional troubleshooting logging at DEBUG log level
      debug-logging = off

      # Maximum number of elements emitted in batch if downstream signals large demand
      output-burst-limit = 1000

      # Enable automatic fusing of all graphs that are run. For short-lived streams
      # this may cause an initial runtime overhead, but most of the time fusing is
      # desirable since it reduces the number of Actors that are created.
      # Deprecated, since Akka 2.5.0, setting does not have any effect.
      auto-fusing = on

      # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,
      # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed
      # buffer upon stream materialization if the requested buffer size is less than this
      # configuration parameter. The default is very high because failing early is better
      # than failing under load.
      #
      # Buffers sized larger than this will dynamically grow/shrink and consume more memory
      # per element than the fixed size buffers.
      max-fixed-buffer-size = 1000000000

      # Maximum number of sync messages that actor can process for stream to substream communication.
      # Parameter allows to interrupt synchronous processing to get upstream/downstream messages.
      # Allows to accelerate message processing that happening within same actor but keep system responsive.
      sync-processing-limit = 1000

      debug {
        # Enables the fuzzing mode which increases the chance of race conditions
        # by aggressively reordering events and making certain operations more
        # concurrent than usual.
        # This setting is for testing purposes, NEVER enable this in a production
        # environment!
        # To get the best results, try combining this setting with a throughput
        # of 1 on the corresponding dispatchers.
        fuzzing-mode = off
      }

      io.tcp {
        # The outgoing bytes are accumulated in a buffer while waiting for acknowledgment
        # of pending write. This improves throughput for small messages (frames) without
        # sacrificing latency. While waiting for the ack the stage will eagerly pull
        # from upstream until the buffer exceeds this size. That means that the buffer may hold
        # slightly more bytes than this limit (at most one element more). It can be set to 0
        # to disable the usage of the buffer.
        write-buffer-size = 16 KiB
      }

      # Time to wait for async materializer creation before throwing an exception
      creation-timeout = 20 seconds

      //#stream-ref
      # configure defaults for SourceRef and SinkRef
      stream-ref {
        # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref
        #
        # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,
        # because the delay of requesting over network boundaries is much higher.
        buffer-capacity = 32

        # Demand is signalled by sending a cumulative demand message ("requesting messages until the n-th sequence number)
        # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should
        # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).
        #
        # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.
        #
        # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive
        # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.
        demand-redelivery-interval = 1 second

        # Subscription timeout, during which the "remote side" MUST subscribe (materialize) the handed out stream ref.
        # This timeout does not have to be very low in normal situations, since the remote side may also need to
        # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking
        # in-active streams which are never subscribed to.
        subscription-timeout = 30 seconds

        # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed
        # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.
        # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the
        # other side of the stream ref would never send the "final" terminal message.
        #
        # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef
        # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.
        final-termination-signal-deadline = 2 seconds
      }
      //#stream-ref
    }

    # Deprecated, left here to not break Akka HTTP which refers to it
    blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"

    # Deprecated, will not be used unless user code refer to it, use 'akka.stream.materializer.blocking-io-dispatcher'
    # instead, or if from code, prefer the 'ActorAttributes.IODispatcher' attribute
    default-blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"
  }

  # configure overrides to ssl-configuration here (to be used by akka-streams, and akka-http – i.e. when serving https connections)
  ssl-config {
    protocol = "TLSv1.2"
  }

  actor {

    serializers {
      akka-stream-ref = "akka.stream.serialization.StreamRefSerializer"
    }

    serialization-bindings {
      "akka.stream.SinkRef"                           = akka-stream-ref
      "akka.stream.SourceRef"                         = akka-stream-ref
      "akka.stream.impl.streamref.StreamRefsProtocol" = akka-stream-ref
    }

    serialization-identifiers {
      "akka.stream.serialization.StreamRefSerializer" = 30
    }
  }
}

# ssl configuration
# folded in from former ssl-config-akka module
ssl-config {
  logger = "com.typesafe.sslconfig.akka.util.AkkaLoggerBridge"
}
# Copyright (C) Lightbend Inc. <https://www.lightbend.com>

# Reference configuration for Play

#default timeout for promises
# @richdougherty: Is this used any more?
promise.akka.actor.typed.timeout=5s

play {
  # Defines whether the global application is allowed
  # Set this to true if you need to use Play.application, or any deprecated global helpers.
  allowGlobalApplication = false

  logger {
    # This is a boolean configuration.
    # If true, the configuration properties will be used when configuring the logger.
    includeConfigProperties = false
  }

  http {

    # The application context.
    # Must start with /.
    context = "/"

    # The error handler.
    # Used by Play's built in DI support to locate and bind a request handler.  Must be the FQCN of a Play router.
    # If null, will attempt to load a class called Routes in the root package, otherwise if that's not found, an empty
    # router will be bound.
    router = null

    # The request handler.
    # Used by Play's built in DI support to locate and bind a request handler.  Must be one of the following:
    # - A FQCN that implements play.api.http.HttpRequestHandler (Scala).
    # - A FQCN that implements play.http.HttpRequestHandler (Java).
    # - provided, indicates that the application has bound an instance of play.api.http.HttpRequestHandler through some
    #   other mechanism.
    # If null, will attempt to load a class called RequestHandler in the root package, otherwise if that's
    # not found, will default to play.api.http.JavaCompatibleHttpRequestHandler.
    requestHandler = null

    # The request handler.
    # Used by Play's built in DI support to locate and bind a request handler.  Must be one of the following:
    # - A FQCN that implements play.http.ActionCreator (Java).
    # If null, will attempt to load a class called ActionCreator in the root package, otherwise if that's
    # not found, will default to play.http.DefaultActionCreator.
    actionCreator = null

    # The error handler.
    # Used by Play's built in DI support to locate and bind an error handler.  Must be one of the following:
    # - A FQCN that implements play.api.http.HttpErrorHandler (Scala).
    # - A FQCN that implements play.http.HttpErrorHandler (Java).
    # - provided, indicates that the application has bound an instance of play.api.http.HttpErrorHandler through some
    #   other mechanism.
    # If null, will attempt to load a class called ErrorHandler in the root package, otherwise if that's
    # not found, will default to play.api.http.DefaultHttpErrorHandler.
    errorHandler = null

    # The filters.
    # Used by Play's built in DI support to locate and bind a class to provide filters.  Must be one of the following:
    # - A FQCN that implements play.api.http.HttpFilters (Scala).
    # - A FQCN that implements play.http.HttpFilters (Java).
    # - provided, indicates that the application has bound an instance of play.api.http.HttpFilters through some
    #   other mechanism.
    # If null, will attempt to load a class called Filters in the root package, otherwise if that's not found, will
    # default to play.api.http.EnabledFilters, which provides the default filters.
    # To disable filters completely, set this to "play.api.http.NoHttpFilters"
    filters = null

    # Forwarded header configuration
    # Play supports various forwarded headers used by proxies to indicate the incoming IP address and protocol of
    # requests. Play uses this in the implementation of the RequestHeader.remoteAddress and RequestHeader.secure
    # fields.
    forwarded = {

      # The version of forwarded headers to use.
      # Valid values are x-forwarded and rfc7239.
      # x-forwarded uses the de facto standard X-Forwarded-For and X-Forwarded-Proto headers to determine the correct
      # remote address and protocol for the request. These headers are widely used, however, they have some serious
      # limitations, for example, if you have multiple proxies, and only one of them adds the X-Forwarded-Proto header,
      # it's impossible to reliably determine which proxy added it and therefore whether the request from the client
      # was made using https or http. rfc7239 uses the new Forwarded header standard, and solves many of the
      # limitations of the X-Forwarded-* headers.
      version = "x-forwarded"

      # The trusted proxies.
      # Trusted proxies may either be individual IPv4 or IPv6 addresses, or be IPv4 or IPv6 CIDR address ranges.
      # This is used to prevent IP address spoofing. Multiple proxies can add and append to the forwarded headers,
      # including the client, which could masquerade as a proxy proxying requests on behalf of another client.  Play
      # will validate that the incoming request IP, and all forwarded headers match the addresses in this list, and will
      # present the first untrusted IP address that it finds (or if all addresses are trusted, the last address) to the
      # application.
      # Note that a number of cloud hosting platforms, most notably AWS, make no guarantees as to what IP addresses
      # their proxies will make requests from. If this is the case, in order for Play to respect the forwarded headers,
      # you need to trust all IP addresses, therefore making it possible for clients to spoof the incoming address.
      # To trust all IP addresses, set this to ["0.0.0.0/0", "::/0"].
      trustedProxies = ["127.0.0.1", "::1"]
    }

    # Parsing configuration
    parser = {

      # The maximum amount of a request body that should be buffered into memory
      maxMemoryBuffer = 100k

      # The maximum amount of a request body that should be buffered into disk
      maxDiskBuffer = 10m
    }

    # Action composition configuration
    actionComposition = {

      # If annotations put directly on Controller classes should be executed before the ones put on action methods
      controllerAnnotationsFirst = false

      # If the action returned by the action creator should be executed before the action composition ones
      executeActionCreatorActionFirst = false
    }

    # Cookies configuration
    cookies = {

      # Whether strict cookie parsing should be used. If true, will ignore the entire cookie header if a single invalid
      # cookie is found, otherwise, will just ignore the invalid cookie if an invalid cookie is found. The reason
      # dropping the entire header may be useful is that browsers don't make any attempt to validate cookie values,
      # which may open opportunities for an attacker to trigger some edge case in the parser to steal cookie
      # information. By dropping the entire header, this makes it harder to exploit edge cases.
      strict = true
    }

    # #session-configuration
    # Session configuration
    session = {

      # The cookie name
      cookieName = "PLAY_SESSION"

      # Whether the secure attribute of the cookie should be set to true
      secure = false

      # The max age to set on the cookie.
      # If null, the cookie expires when the user closes their browser.
      # An important thing to note, this only sets when the browser will discard the cookie.
      maxAge = null

      # Whether the HTTP only attribute of the cookie should be set to true
      httpOnly = true

      # The value of the SameSite attribute of the cookie. Set to null for no SameSite attribute.
      # Possible values are "lax" and "strict". If misconfigured it's set to null.
      sameSite = "lax"

      # The domain to set on the session cookie
      # If null, does not set a domain on the session cookie.
      domain = null

      # The session path
      # Must start with /.
      path = ${play.http.context}

      jwt {
        # The JWT signature algorithm to use on the session cookie
        # uses 'alg' https://tools.ietf.org/html/rfc7515#section-4.1.1
        signatureAlgorithm = "HS256"

        # The time after which the session is automatically invalidated.
        # Use 'exp' https://tools.ietf.org/html/rfc7519#section-4.1.4
        expiresAfter = ${play.http.session.maxAge}

        # The amount of clock skew to accept between servers when performing date checks
        # If you have NTP or roughtime synchronizing between servers, you can enhance
        # security by tightening this value.
        clockSkew = 5 minutes

        # The claim key under which all user data is stored in the JWT.
        dataClaim = "data"
      }
    }
    # #session-configuration

    # Flash configuration
    flash = {
      # The cookie name
      cookieName = "PLAY_FLASH"

      # Whether the flash cookie should be secure or not
      secure = false

      # Whether the HTTP only attribute of the cookie should be set to true
      httpOnly = true

      # The value of the SameSite attribute of the cookie. Set to null for no SameSite attribute.
      # Possible values are "lax" and "strict". If misconfigured it's set to null.
      sameSite = "lax"

      # The flash path
      # Must start with /.
      path = ${play.http.context}

      # The domain to set on the flash cookie
      # If null, does not set a domain on the flash cookie.
      domain = ${play.http.session.domain}

      jwt {
        # The JWT signature algorithm to use on the session cookie
        # uses 'alg' https://tools.ietf.org/html/rfc7515#section-4.1.1
        signatureAlgorithm = "HS256"

        # The time after which the session is automatically invalidated.
        # Use 'exp' https://tools.ietf.org/html/rfc7519#section-4.1.4
        expiresAfter = null

        # The amount of clock skew to accept between servers when performing date checks
        # If you have NTP or roughtime synchronizing between servers, you can enhance
        # security by tightening this value.
        clockSkew = 5 minutes

        # The claim key under which all user data is stored in the JWT.
        dataClaim = "data"
      }
    }

    # Secret configuration
    secret {
      # The application secret. Must be set. A value of "changeme" will cause the application to fail to start in
      # production.
      key = "changeme"

      # The JCE provider to use. If null, uses the platform default.
      provider = null
    }

    fileMimeTypes = """
        3dm=x-world/x-3dmf
        3dmf=x-world/x-3dmf
        7z=application/x-7z-compressed
        a=application/octet-stream
        aab=application/x-authorware-bin
        aam=application/x-authorware-map
        aas=application/x-authorware-seg
        abc=text/vndabc
        ace=application/x-ace-compressed
        acgi=text/html
        afl=video/animaflex
        ai=application/postscript
        aif=audio/aiff
        aifc=audio/aiff
        aiff=audio/aiff
        aim=application/x-aim
        aip=text/x-audiosoft-intra
        alz=application/x-alz-compressed
        ani=application/x-navi-animation
        aos=application/x-nokia-9000-communicator-add-on-software
        aps=application/mime
        arc=application/x-arc-compressed
        arj=application/arj
        art=image/x-jg
        asf=video/x-ms-asf
        asm=text/x-asm
        asp=text/asp
        asx=application/x-mplayer2
        au=audio/basic
        avi=video/x-msvideo
        avs=video/avs-video
        bcpio=application/x-bcpio
        bin=application/mac-binary
        bmp=image/bmp
        boo=application/book
        book=application/book
        boz=application/x-bzip2
        bsh=application/x-bsh
        bz2=application/x-bzip2
        bz=application/x-bzip
        c++=text/plain
        c=text/x-c
        cab=application/vnd.ms-cab-compressed
        cat=application/vndms-pkiseccat
        cc=text/x-c
        ccad=application/clariscad
        cco=application/x-cocoa
        cdf=application/cdf
        cer=application/pkix-cert
        cha=application/x-chat
        chat=application/x-chat
        chrt=application/vnd.kde.kchart
        class=application/java
        # ? class=application/java-vm
        com=text/plain
        conf=text/plain
        cpio=application/x-cpio
        cpp=text/x-c
        cpt=application/mac-compactpro
        crl=application/pkcs-crl
        crt=application/pkix-cert
        crx=application/x-chrome-extension
        csh=text/x-scriptcsh
        csp=application/csp-report
        css=text/css
        csv=text/csv
        cxx=text/plain
        dar=application/x-dar
        dcr=application/x-director
        deb=application/x-debian-package
        deepv=application/x-deepv
        def=text/plain
        der=application/x-x509-ca-cert
        dfont=application/x-font-ttf
        dif=video/x-dv
        dir=application/x-director
        divx=video/divx
        dl=video/dl
        dmg=application/x-apple-diskimage
        doc=application/msword
        dot=application/msword
        dp=application/commonground
        drw=application/drafting
        dump=application/octet-stream
        dv=video/x-dv
        dvi=application/x-dvi
        dwf=drawing/x-dwf=(old)
        dwg=application/acad
        dxf=application/dxf
        dxr=application/x-director
        el=text/x-scriptelisp
        elc=application/x-bytecodeelisp=(compiled=elisp)
        eml=message/rfc822
        env=application/x-envoy
        eot=application/vnd.ms-fontobject
        eps=application/postscript
        es=application/x-esrehber
        etx=text/x-setext
        evy=application/envoy
        exe=application/octet-stream
        f77=text/x-fortran
        f90=text/x-fortran
        f=text/x-fortran
        fdf=application/vndfdf
        fif=application/fractals
        fli=video/fli
        flo=image/florian
        flv=video/x-flv
        flx=text/vndfmiflexstor
        fmf=video/x-atomic3d-feature
        for=text/x-fortran
        fpx=image/vndfpx
        frl=application/freeloader
        funk=audio/make
        g3=image/g3fax
        g=text/plain
        gif=image/gif
        gl=video/gl
        gsd=audio/x-gsm
        gsm=audio/x-gsm
        gsp=application/x-gsp
        gss=application/x-gss
        gtar=application/x-gtar
        gz=application/x-compressed
        gzip=application/x-gzip
        h=text/x-h
        hdf=application/x-hdf
        help=application/x-helpfile
        hgl=application/vndhp-hpgl
        hh=text/x-h
        hlb=text/x-script
        hlp=application/hlp
        hpg=application/vndhp-hpgl
        hpgl=application/vndhp-hpgl
        hqx=application/binhex
        hta=application/hta
        htc=text/x-component
        htm=text/html
        html=text/html
        htmls=text/html
        htt=text/webviewhtml
        htx=text/html
        ice=x-conference/x-cooltalk
        ico=image/x-icon
        ics=text/calendar
        icz=text/calendar
        idc=text/plain
        ief=image/ief
        iefs=image/ief
        iges=application/iges
        igs=application/iges
        ima=application/x-ima
        imap=application/x-httpd-imap
        inf=application/inf
        ins=application/x-internett-signup
        ip=application/x-ip2
        isu=video/x-isvideo
        it=audio/it
        iv=application/x-inventor
        ivr=i-world/i-vrml
        ivy=application/x-livescreen
        jam=audio/x-jam
        jav=text/x-java-source
        java=text/x-java-source
        jcm=application/x-java-commerce
        jfif-tbnl=image/jpeg
        jfif=image/jpeg
        jnlp=application/x-java-jnlp-file
        jpe=image/jpeg
        jpeg=image/jpeg
        jpg=image/jpeg
        jps=image/x-jps
        js=application/javascript
        json=application/json
        jut=image/jutvision
        kar=audio/midi
        karbon=application/vnd.kde.karbon
        kfo=application/vnd.kde.kformula
        flw=application/vnd.kde.kivio
        kml=application/vnd.google-earth.kml+xml
        kmz=application/vnd.google-earth.kmz
        kon=application/vnd.kde.kontour
        kpr=application/vnd.kde.kpresenter
        kpt=application/vnd.kde.kpresenter
        ksp=application/vnd.kde.kspread
        kwd=application/vnd.kde.kword
        kwt=application/vnd.kde.kword
        ksh=text/x-scriptksh
        la=audio/nspaudio
        lam=audio/x-liveaudio
        latex=application/x-latex
        lha=application/lha
        lhx=application/octet-stream
        list=text/plain
        lma=audio/nspaudio
        log=text/plain
        lsp=text/x-scriptlisp
        lst=text/plain
        lsx=text/x-la-asf
        ltx=application/x-latex
        lzh=application/octet-stream
        lzx=application/lzx
        m1v=video/mpeg
        m2a=audio/mpeg
        m2v=video/mpeg
        m3u=audio/x-mpegurl
        m=text/x-m
        man=application/x-troff-man
        manifest=text/cache-manifest
        map=application/x-navimap
        mar=text/plain
        mbd=application/mbedlet
        mc$=application/x-magic-cap-package-10
        mcd=application/mcad
        mcf=text/mcf
        mcp=application/netmc
        me=application/x-troff-me
        mht=message/rfc822
        mhtml=message/rfc822
        mid=application/x-midi
        midi=application/x-midi
        mif=application/x-frame
        mime=message/rfc822
        mjf=audio/x-vndaudioexplosionmjuicemediafile
        mjpg=video/x-motion-jpeg
        mm=application/base64
        mme=application/base64
        mod=audio/mod
        moov=video/quicktime
        mov=video/quicktime
        movie=video/x-sgi-movie
        mp2=audio/mpeg
        mp3=audio/mpeg
        mp4=video/mp4
        mpa=audio/mpeg
        mpc=application/x-project
        mpe=video/mpeg
        mpeg=video/mpeg
        mpg=video/mpeg
        mpga=audio/mpeg
        mpp=application/vndms-project
        mpt=application/x-project
        mpv=application/x-project
        mpx=application/x-project
        mrc=application/marc
        ms=application/x-troff-ms
        mv=video/x-sgi-movie
        my=audio/make
        mzz=application/x-vndaudioexplosionmzz
        nap=image/naplps
        naplps=image/naplps
        nc=application/x-netcdf
        ncm=application/vndnokiaconfiguration-message
        nif=image/x-niff
        niff=image/x-niff
        nix=application/x-mix-transfer
        nsc=application/x-conference
        nvd=application/x-navidoc
        o=application/octet-stream
        oda=application/oda
        odb=application/vnd.oasis.opendocument.database
        odc=application/vnd.oasis.opendocument.chart
        odf=application/vnd.oasis.opendocument.formula
        odg=application/vnd.oasis.opendocument.graphics
        odi=application/vnd.oasis.opendocument.image
        odm=application/vnd.oasis.opendocument.text-master
        odp=application/vnd.oasis.opendocument.presentation
        ods=application/vnd.oasis.opendocument.spreadsheet
        odt=application/vnd.oasis.opendocument.text
        oga=audio/ogg
        ogg=audio/ogg
        ogv=video/ogg
        omc=application/x-omc
        omcd=application/x-omcdatamaker
        omcr=application/x-omcregerator
        otc=application/vnd.oasis.opendocument.chart-template
        otf=application/vnd.oasis.opendocument.formula-template
        otg=application/vnd.oasis.opendocument.graphics-template
        oth=application/vnd.oasis.opendocument.text-web
        oti=application/vnd.oasis.opendocument.image-template
        otm=application/vnd.oasis.opendocument.text-master
        otp=application/vnd.oasis.opendocument.presentation-template
        ots=application/vnd.oasis.opendocument.spreadsheet-template
        ott=application/vnd.oasis.opendocument.text-template
        p10=application/pkcs10
        p12=application/pkcs-12
        p7a=application/x-pkcs7-signature
        p7c=application/pkcs7-mime
        p7m=application/pkcs7-mime
        p7r=application/x-pkcs7-certreqresp
        p7s=application/pkcs7-signature
        p=text/x-pascal
        part=application/pro_eng
        pas=text/pascal
        pbm=image/x-portable-bitmap
        pcl=application/vndhp-pcl
        pct=image/x-pict
        pcx=image/x-pcx
        pdb=chemical/x-pdb
        pdf=application/pdf
        pfunk=audio/make
        pgm=image/x-portable-graymap
        pic=image/pict
        pict=image/pict
        pkg=application/x-newton-compatible-pkg
        pko=application/vndms-pkipko
        pl=text/x-scriptperl
        plx=application/x-pixclscript
        pm4=application/x-pagemaker
        pm5=application/x-pagemaker
        pm=text/x-scriptperl-module
        png=image/png
        pnm=application/x-portable-anymap
        pot=application/mspowerpoint
        pov=model/x-pov
        ppa=application/vndms-powerpoint
        ppm=image/x-portable-pixmap
        pps=application/mspowerpoint
        ppt=application/mspowerpoint
        ppz=application/mspowerpoint
        pre=application/x-freelance
        prt=application/pro_eng
        ps=application/postscript
        psd=application/octet-stream
        pvu=paleovu/x-pv
        pwz=application/vndms-powerpoint
        py=text/x-scriptphyton
        pyc=application/x-bytecodepython
        qcp=audio/vndqcelp
        qd3=x-world/x-3dmf
        qd3d=x-world/x-3dmf
        qif=image/x-quicktime
        qt=video/quicktime
        qtc=video/x-qtc
        qti=image/x-quicktime
        qtif=image/x-quicktime
        ra=audio/x-pn-realaudio
        ram=audio/x-pn-realaudio
        rar=application/x-rar-compressed
        ras=application/x-cmu-raster
        rast=image/cmu-raster
        rdf=application/rdf+xml
        rexx=text/x-scriptrexx
        rf=image/vndrn-realflash
        rgb=image/x-rgb
        rm=application/vndrn-realmedia
        rmi=audio/mid
        rmm=audio/x-pn-realaudio
        rmp=audio/x-pn-realaudio
        rng=application/ringing-tones
        rnx=application/vndrn-realplayer
        roff=application/x-troff
        rp=image/vndrn-realpix
        rpm=audio/x-pn-realaudio-plugin
        rt=text/vndrn-realtext
        rtf=application/rtf
        rtx=application/rtx
        rv=video/vndrn-realvideo
        s=text/x-asm
        s3m=audio/s3m
        s7z=application/x-7z-compressed
        saveme=application/octet-stream
        sbk=application/x-tbook
        scm=text/x-scriptscheme
        sdml=text/plain
        sdp=application/sdp
        sdr=application/sounder
        sea=application/sea
        set=application/set
        sgm=text/x-sgml
        sgml=text/x-sgml
        sh=text/x-scriptsh
        shar=application/x-bsh
        shtml=text/x-server-parsed-html
        sid=audio/x-psid
        skd=application/x-koan
        skm=application/x-koan
        skp=application/x-koan
        skt=application/x-koan
        sit=application/x-stuffit
        sitx=application/x-stuffitx
        sl=application/x-seelogo
        smi=application/smil
        smil=application/smil
        snd=audio/basic
        sol=application/solids
        spc=text/x-speech
        spl=application/futuresplash
        spr=application/x-sprite
        sprite=application/x-sprite
        spx=audio/ogg
        src=application/x-wais-source
        ssi=text/x-server-parsed-html
        ssm=application/streamingmedia
        sst=application/vndms-pkicertstore
        step=application/step
        stl=application/sla
        stp=application/step
        sv4cpio=application/x-sv4cpio
        sv4crc=application/x-sv4crc
        svf=image/vnddwg
        svg=image/svg+xml
        svr=application/x-world
        swf=application/x-shockwave-flash
        t=application/x-troff
        talk=text/x-speech
        tar=application/x-tar
        tbk=application/toolbook
        tcl=text/x-scripttcl
        tcsh=text/x-scripttcsh
        tex=application/x-tex
        texi=application/x-texinfo
        texinfo=application/x-texinfo
        text=text/plain
        tgz=application/gnutar
        tif=image/tiff
        tiff=image/tiff
        tr=application/x-troff
        tsi=audio/tsp-audio
        tsp=application/dsptype
        tsv=text/tab-separated-values
        turbot=image/florian
        tte=application/x-font-ttf
        ttf=application/x-font-ttf
        ttl=text/turtle
        txt=text/plain
        uil=text/x-uil
        uni=text/uri-list
        unis=text/uri-list
        unv=application/i-deas
        uri=text/uri-list
        uris=text/uri-list
        ustar=application/x-ustar
        uu=text/x-uuencode
        uue=text/x-uuencode
        vcd=application/x-cdlink
        vcf=text/x-vcard
        vcard=text/x-vcard
        vcs=text/x-vcalendar
        vda=application/vda
        vdo=video/vdo
        vew=application/groupwise
        viv=video/vivo
        vivo=video/vivo
        vmd=application/vocaltec-media-desc
        vmf=application/vocaltec-media-file
        voc=audio/voc
        vos=video/vosaic
        vox=audio/voxware
        vqe=audio/x-twinvq-plugin
        vqf=audio/x-twinvq
        vql=audio/x-twinvq-plugin
        vrml=application/x-vrml
        vrt=x-world/x-vrt
        vsd=application/x-visio
        vst=application/x-visio
        vsw=application/x-visio
        w60=application/wordperfect60
        w61=application/wordperfect61
        w6w=application/msword
        wav=audio/wav
        wb1=application/x-qpro
        wbmp=image/vnd.wap.wbmp
        web=application/vndxara
        webm=video/webm
        wiz=application/msword
        wk1=application/x-123
        wmf=windows/metafile
        wml=text/vnd.wap.wml
        wmlc=application/vnd.wap.wmlc
        wmls=text/vnd.wap.wmlscript
        wmlsc=application/vnd.wap.wmlscriptc
        woff=application/font-woff
        woff2=application/font-woff2
        word=application/msword
        wp5=application/wordperfect
        wp6=application/wordperfect
        wp=application/wordperfect
        wpd=application/wordperfect
        wq1=application/x-lotus
        wri=application/mswrite
        wrl=application/x-world
        wrz=model/vrml
        wsc=text/scriplet
        wsrc=application/x-wais-source
        wtk=application/x-wintalk
        x-png=image/png
        xbm=image/x-xbitmap
        xdr=video/x-amt-demorun
        xgz=xgl/drawing
        xif=image/vndxiff
        xl=application/excel
        xla=application/excel
        xlb=application/excel
        xlc=application/excel
        xld=application/excel
        xlk=application/excel
        xll=application/excel
        xlm=application/excel
        xls=application/excel
        xlt=application/excel
        xlv=application/excel
        xlw=application/excel
        xm=audio/xm
        xml=application/xml
        xmz=xgl/movie
        xpi=application/x-xpinstall
        xpix=application/x-vndls-xpix
        xpm=image/x-xpixmap
        xsr=video/x-amt-showrun
        xwd=image/x-xwd
        xyz=chemical/x-pdb
        z=application/x-compress
        zip=application/zip
        zoo=application/octet-stream
        zsh=text/x-scriptzsh

        # Office 2007 mess - http://wdg.uncc.edu/Microsoft_Office_2007_MIME_Types_for_Apache_and_IIS
        docx=application/vnd.openxmlformats-officedocument.wordprocessingml.document
        docm=application/vnd.ms-word.document.macroEnabled.12
        dotx=application/vnd.openxmlformats-officedocument.wordprocessingml.template
        dotm=application/vnd.ms-word.template.macroEnabled.12
        xlsx=application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
        xlsm=application/vnd.ms-excel.sheet.macroEnabled.12
        xltx=application/vnd.openxmlformats-officedocument.spreadsheetml.template
        xltm=application/vnd.ms-excel.template.macroEnabled.12
        xlsb=application/vnd.ms-excel.sheet.binary.macroEnabled.12
        xlam=application/vnd.ms-excel.addin.macroEnabled.12
        pptx=application/vnd.openxmlformats-officedocument.presentationml.presentation
        pptm=application/vnd.ms-powerpoint.presentation.macroEnabled.12
        ppsx=application/vnd.openxmlformats-officedocument.presentationml.slideshow
        ppsm=application/vnd.ms-powerpoint.slideshow.macroEnabled.12
        potx=application/vnd.openxmlformats-officedocument.presentationml.template
        potm=application/vnd.ms-powerpoint.template.macroEnabled.12
        ppam=application/vnd.ms-powerpoint.addin.macroEnabled.12
        sldx=application/vnd.openxmlformats-officedocument.presentationml.slide
        sldm=application/vnd.ms-powerpoint.slide.macroEnabled.12
        thmx=application/vnd.ms-officetheme
        onetoc=application/onenote
        onetoc2=application/onenote
        onetmp=application/onenote
        onepkg=application/onenote
        # koffice

        # iWork
        key=application/x-iwork-keynote-sffkey
        kth=application/x-iwork-keynote-sffkth
        nmbtemplate=application/x-iwork-numbers-sfftemplate
        numbers=application/x-iwork-numbers-sffnumbers
        pages=application/x-iwork-pages-sffpages
        template=application/x-iwork-pages-sfftemplate

        # Extensions for Mozilla apps (Firefox and friends)
        xpi=application/x-xpinstall
      """
  }

  filters {
    # List of enabled filters as fully qualified class names
    # enabled = []

    # List of disabled filters as fully qualified class names
    disabled = []
  }

  temporaryFile {
    # Path to directory where temporary files will be stored
    dir = ${?java.io.tmpdir}

    # Removes stale temporary files from the filesystem.  This is a backup
    # to the "remove-on-gc" functionality in the default temporary file creator,
    # for when GC is not happening fast enough.  Uses play.akka.blockingIoDispatcher.
    reaper {
      enabled = false
      initialDelay = "5 minutes"
      interval = "5 minutes"
      olderThan = "5 minutes"
    }
  }

  # The ApplicationLoader to use for creating the Application.
  # This MUST either be set in application.conf or in some module.
  #application.loader = null

  modules {

    # The enabled modules that should be automatically loaded.
    enabled += "play.api.inject.BuiltinModule"
    enabled += "play.api.i18n.I18nModule"
    enabled += "play.api.mvc.CookiesModule"
    enabled += "controllers.AssetsModule"

    # A way to disable modules that are automatically enabled
    disabled = []

  }

  # Internationalisation configuration
  i18n {

    # The languages supported by this application
    langs = []

    # A path to prefix message file loading with.  Use this if you want to place your messages resources at some path
    # other than the root application path.
    path = null

    # The name of the cookie to store the Play language in.  This cookie is set when MessagesApi.setLang is invoked, and
    # read when the preferred lang is loaded.
    langCookieName = "PLAY_LANG"

    # The max age to set on the cookie.
    # If null, the cookie expires when the user closes their browser.
    # An important thing to note, this only sets when the browser will discard the cookie.
    langCookieMaxAge = null

    # Whether the language cookie should be secure or not
    langCookieSecure = false

    # Whether the HTTP only attribute of the cookie should be set to true
    langCookieHttpOnly = false

    # The value of the SameSite attribute of the cookie. Set to null for no SameSite attribute.
    # Possible values are "lax" and "strict". If misconfigured it's set to null.
    langCookieSameSite = "lax"

  }

  akka {

    # The name of the actor system that Play creates
    actor-system = "application"

    # How long Play should wait for Akka to shutdown before timing it.  If "infinite" or null, waits indefinitely.
    shutdown-timeout = infinite

    # The location to read Play's Akka configuration from
    config = "akka"

    # The blocking IO dispatcher, used for serving files/resources from the file system or classpath.
    blockingIoDispatcher {
      fork-join-executor {
        parallelism-factor = 3.0
      }

    }

    # The dev mode actor system. Play typically uses the application actor system, however, in dev mode, an actor
    # system is needed that outlives the application actor system, since the HTTP server will need to use this, and it
    # lives through many application (and therefore actor system) restarts.
    dev-mode {
      # Turn off dead letters until Akka HTTP server is stable
      log-dead-letters = off

      # Disable Akka-HTTP's transparent HEAD handling. so that play's HEAD handling can take action
      http.server.transparent-head-requests = false

      akka {

        # Since Akka 2.5.8 there's a setting to disable all Akka-provided JVM shutdown
        # hooks. This will not only enable CoordinatedShutdown but also Artery or other
        # Akka-managed hooks.
        jvm-shutdown-hooks = on

        # CoordinatedShutdown is an extension introduced in Akka 2.5 that will
        # perform registered tasks in the order that is defined by the phases.
        # This setup override Akka default settings with Play-specific ones for dev mode.
        coordinated-shutdown {

          # Terminate the ActorSystem in the last phase actor-system-terminate.
          terminate-actor-system = on

          # This setting is on the `dev-mode` specific settings and is only used by the Server. It
          # doesn't make sense to exit the JVM in Dev mode.
          exit-jvm = off

          # Run the coordinated shutdown when the JVM process exits, e.g.
          # via kill SIGTERM signal. This setting is on the `dev-mode` specific settings and is
          # only used by the Server. Defaults to 'on' since exiting the build tool (sbt/gradle/...)
          # should cause the execution of `coordinated-shutdown`.
          run-by-jvm-shutdown-hook = on
        }
      }
    }
  }

  #Assets configuration
  assets {

    # The path on the classpath where assets are located (should be the same as the path parameter in route)
    path = "/public"
    # The URL prefix before your asset name (excluding the trailing slash)
    urlPrefix = "/assets"

    #Default behaviour for checkForMinified is false for dev and true for non-dev modes
    checkForMinified = null

    defaultCache = "public, max-age=3600"

    aggressiveCache = "public, max-age=31536000, immutable"

    digest.algorithm = "md5"

    default.charset = "utf-8"

    # registrations which have charset="utf-8" appended to the content-type header.
    textContentTypes = [ "application/json", "application/javascript" ]

    # This defines which compressions of assets are served by the Assets controller
    # and which priorities they have. E.g. having "br" as first entry and "gzip" as
    # second one will serve a brotli compressed asset rather than a gzip compressed
    # asset to a client supporting both compressions.
    #
    # It also defines for which kind of compressed assets we're looking for on the classpath.
    # If you know, you only provide certain kinds of compressions, disable the others to get
    # a little bit more performance out of your application.
    encodings = [
      { accept: "br", extension: "br"}
      { accept: "gzip", extension: "gz" }
      { accept: "xz", extension: "xz" }
      { accept: "bz2", extension: "bz2" }
    ]

  }

}
akka.actor.typed {

  # List FQCN of `akka.actor.typed.ExtensionId`s which shall be loaded at actor system startup.
  # Should be on the format: 'extensions = ["com.example.MyExtId1", "com.example.MyExtId2"]' etc.
  # See the Akka Documentation for more info about Extensions
  extensions = []

  # List FQCN of extensions which shall be loaded at actor system startup.
  # Library extensions are regular extensions that are loaded at startup and are
  # available for third party library authors to enable auto-loading of extensions when
  # present on the classpath. This is done by appending entries:
  # 'library-extensions += "Extension"' in the library `reference.conf`.
  #
  # Should not be set by end user applications in 'application.conf', use the extensions property for that
  #
  library-extensions = ${?akka.actor.typed.library-extensions} []

  # Receptionist is started eagerly to allow clustered receptionist to gather remote registrations early on.
  library-extensions += "akka.actor.typed.receptionist.Receptionist$"

  # While an actor is restarted (waiting for backoff to expire and children to stop)
  # incoming messages and signals are stashed, and delivered later to the newly restarted
  # behavior. This property defines the capacity in number of messages of the stash
  # buffer. If the capacity is exceed then additional incoming messages are dropped.
  restart-stash-capacity = 1000

  # Typed mailbox defaults to the single consumber mailbox as balancing dispatcher is not supported
  default-mailbox {
    mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
  }
}

# Load typed extensions by a classic extension.
akka.library-extensions += "akka.actor.typed.internal.adapter.ActorSystemAdapter$LoadTypedExtensions"

akka.actor {
  serializers {
    typed-misc = "akka.actor.typed.internal.MiscMessageSerializer"
    service-key = "akka.actor.typed.internal.receptionist.ServiceKeySerializer"
  }

  serialization-identifiers {
    "akka.actor.typed.internal.MiscMessageSerializer" = 24
    "akka.actor.typed.internal.receptionist.ServiceKeySerializer" = 26
  }

  serialization-bindings {
    "akka.actor.typed.ActorRef" = typed-misc
    "akka.actor.typed.internal.adapter.ActorRefAdapter" = typed-misc
    "akka.actor.typed.internal.receptionist.DefaultServiceKey" = service-key
  }
}

# When using Akka Typed (having akka-actor-typed in classpath) the
# akka.event.slf4j.Slf4jLogger is enabled instead of the DefaultLogger
# even though it has not been explicitly defined in `akka.loggers`
# configuration.
#
# Slf4jLogger will be used for all Akka classic logging via eventStream,
# including logging from Akka internals. The Slf4jLogger is then using
# an ordinary org.slf4j.Logger to emit the log events.
#
# The Slf4jLoggingFilter is also enabled automatically.
#
# This behavior can be disabled by setting this property to `off`.
akka.use-slf4j = on

akka.reliable-delivery {
  producer-controller {
    durable-queue {
      # The ProducerController uses this timeout for the requests to
      # the durable queue. If there is no reply within the timeout it
      # will be retried.
      request-timeout = 3s

      # The ProducerController retries requests to the durable queue this
      # number of times before failing.
      retry-attempts = 10

      # The ProducerController retries sending the first message with this interval
      # until it has been confirmed.
      resend-first-interval = 1s
    }
  }

  consumer-controller {
    # Number of messages in flight between ProducerController and
    # ConsumerController. The ConsumerController requests for more messages
    # when half of the window has been used.
    flow-control-window = 50

    # The ConsumerController resends flow control messages to the
    # ProducerController with the resend-interval-min, and increasing
    # it gradually to resend-interval-max when idle.
    resend-interval-min = 2s
    resend-interval-max = 30s

    # If this is enabled lost messages will not be resent, but flow control is used.
    # This can be more efficient since messages don't have to be
    # kept in memory in the `ProducerController` until they have been
    # confirmed, but the drawback is that lost messages will not be delivered.
    only-flow-control = false
  }

  work-pulling {
    producer-controller = ${akka.reliable-delivery.producer-controller}
    producer-controller {
      # Limit of how many messages that can be buffered when there
      # is no demand from the consumer side.
      buffer-size = 1000

      # Ask timeout for sending message to worker until receiving Ack from worker
      internal-ask-timeout = 60s
    }
  }
}
########################################
# akka-http-core Reference Config File #
########################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

# Akka HTTP version, checked against the runtime version of Akka HTTP.
# Loaded from generated conf file.
include "akka-http-version"

akka.http {

  server {
    # The default value of the `Server` header to produce if no
    # explicit `Server`-header was included in a response.
    # If this value is the empty string and no header was included in
    # the request, no `Server` header will be rendered at all.
    server-header = akka-http/${akka.http.version}

    # "PREVIEW" features that are not yet fully production ready.
    # These flags can change or be removed between patch releases.
    preview {
      # ONLY WORKS WITH `bindAndHandleAsync` (currently)
      #
      # If this setting is enabled AND the akka-http2-support is found
      # on the classpath the usual Http().bind... method calls will bind
      # using HTTP/2. Please note that you must configure HTTPS while doing so.
      enable-http2 = off
    }

    # The time after which an idle connection will be automatically closed.
    # Set to `infinite` to completely disable idle connection timeouts.
    idle-timeout = 60 s

    # Defines the default time period within which the application has to
    # produce an HttpResponse for any given HttpRequest it received.
    # The timeout begins to run when the *end* of the request has been
    # received, so even potentially long uploads can have a short timeout.
    # Set to `infinite` to completely disable request timeout checking.
    #
    # Make sure this timeout is smaller than the idle-timeout, otherwise,
    # the idle-timeout will kick in first and reset the TCP connection
    # without a response.
    #
    # If this setting is not `infinite` the HTTP server layer attaches a
    # `Timeout-Access` header to the request, which enables programmatic
    # customization of the timeout period and timeout response for each
    # request individually.
    request-timeout = 20 s

    # The time period within which the TCP binding process must be completed.
    bind-timeout = 1s

    # Default port to bind HTTP server to when no port was explicitly given.
    default-http-port = 80

    # Default port to bind HTTPS server to when no port was explicitly given.
    default-https-port = 443

    # The time period the HTTP server implementation will keep a connection open after
    # all data has been delivered to the network layer. This setting is similar to the SO_LINGER socket option
    # but does not only include the OS-level socket but also covers the Akka IO / Akka Streams network stack.
    # The setting is an extra precaution that prevents clients from keeping open a connection that is
    # already considered completed from the server side.
    #
    # If the network level buffers (including the Akka Stream / Akka IO networking stack buffers)
    # contains more data than can be transferred to the client in the given time when the server-side considers
    # to be finished with this connection, the client may encounter a connection reset.
    #
    # Set to 'infinite' to disable automatic connection closure (which will risk to leak connections).
    linger-timeout = 1 min

    # The maximum number of concurrently accepted connections when using the
    # `Http().bindAndHandle` methods.
    #
    # This setting doesn't apply to the `Http().bind` method which will still
    # deliver an unlimited backpressured stream of incoming connections.
    #
    # Note, that this setting limits the number of the connections on a best-effort basis.
    # It does *not* strictly guarantee that the number of established TCP connections will never
    # exceed the limit (but it will be approximately correct) because connection termination happens
    # asynchronously. It also does *not* guarantee that the number of concurrently active handler
    # flow materializations will never exceed the limit for the reason that it is impossible to reliably
    # detect when a materialization has ended.
    max-connections = 1024

    # The maximum number of requests that are accepted (and dispatched to
    # the application) on one single connection before the first request
    # has to be completed.
    # Incoming requests that would cause the pipelining limit to be exceeded
    # are not read from the connections socket so as to build up "back-pressure"
    # to the client via TCP flow control.
    # A setting of 1 disables HTTP pipelining, since only one request per
    # connection can be "open" (i.e. being processed by the application) at any
    # time. Set to higher values to enable HTTP pipelining.
    # This value must be > 0 and <= 1024.
    pipelining-limit = 16

    # Enables/disables the addition of a `Remote-Address` header
    # holding the clients (remote) IP address.
    remote-address-header = off

    # Enables/disables the addition of a `Raw-Request-URI` header holding the
    # original raw request URI as the client has sent it.
    raw-request-uri-header = off

    # Enables/disables automatic handling of HEAD requests.
    # If this setting is enabled the server dispatches HEAD requests as GET
    # requests to the application and automatically strips off all message
    # bodies from outgoing responses.
    # Note that, even when this setting is off the server will never send
    # out message bodies on responses to HEAD requests.
    transparent-head-requests = on

    # Enables/disables the returning of more detailed error messages to
    # the client in the error response.
    # Should be disabled for browser-facing APIs due to the risk of XSS attacks
    # and (probably) enabled for internal or non-browser APIs.
    # Note that akka-http will always produce log messages containing the full
    # error details.
    verbose-error-messages = off

    # The initial size of the buffer to render the response headers in.
    # Can be used for fine-tuning response rendering performance but probably
    # doesn't have to be fiddled with in most applications.
    response-header-size-hint = 512

    # The requested maximum length of the queue of incoming connections.
    # If the server is busy and the backlog is full the OS will start dropping
    # SYN-packets and connection attempts may fail. Note, that the backlog
    # size is usually only a maximum size hint for the OS and the OS can
    # restrict the number further based on global limits.
    backlog = 100

    # If this setting is empty the server only accepts requests that carry a
    # non-empty `Host` header. Otherwise it responds with `400 Bad Request`.
    # Set to a non-empty value to be used in lieu of a missing or empty `Host`
    # header to make the server accept such requests.
    # Note that the server will never accept HTTP/1.1 request without a `Host`
    # header, i.e. this setting only affects HTTP/1.1 requests with an empty
    # `Host` header as well as HTTP/1.0 requests.
    # Examples: `www.spray.io` or `example.com:8080`
    default-host-header = ""

    # Socket options to set for the listening socket. If a setting is left
    # undefined, it will use whatever the default on the system is.
    socket-options {
      so-receive-buffer-size = undefined
      so-send-buffer-size = undefined
      so-reuse-address = undefined
      so-traffic-class = undefined
      tcp-keep-alive = undefined
      tcp-oob-inline = undefined
      tcp-no-delay = undefined
    }

    # When graceful termination is enabled and used invoked with a deadline,
    # after the deadline passes pending requests will be replied to with a "terminating" http response,
    # instead of delivering those requests to the user-handler.
    # This response is configurable here using configuration, or via code in case more a sophisticated (e.g. with response entity)
    # response is needed.
    #
    termination-deadline-exceeded-response {
      # Status code of the "terminating" response to be automatically sent to pending requests once the termination deadline is exceeded.
      status = 503 # ServiceUnavailable
    }

    # Modify to tweak parsing settings on the server-side only.
    parsing {
      # no overrides by default, see `akka.http.parsing` for default values
    }

    # Enables/disables the logging of unencrypted HTTP traffic to and from the HTTP
    # server for debugging reasons.
    #
    # Note: Use with care. Logging of unencrypted data traffic may expose secret data.
    #
    # Incoming and outgoing traffic will be logged in hexdump format. To enable logging,
    # specify the number of bytes to log per chunk of data (the actual chunking depends
    # on implementation details and networking conditions and should be treated as
    # arbitrary).
    #
    # For logging on the client side, see akka.http.client.log-unencrypted-network-bytes.
    #
    # `off` : no log messages are produced
    # Int   : determines how many bytes should be logged per data chunk
    log-unencrypted-network-bytes = off

    http2 {
      # The maximum number of request per connection concurrently dispatched to the request handler.
      max-concurrent-streams = 256

      # The maximum number of bytes to receive from a request entity in a single chunk.
      #
      # The reasoning to limit that amount (instead of delivering all buffered data for a stream) is that
      # the amount of data in the internal buffers will drive backpressure and flow control on the HTTP/2 level. Bigger
      # chunks would mean that the user-level entity reader will have to buffer all that data if it cannot read it in one
      # go. The implementation would not be able to backpressure further data in that case because it does not know about
      # this user-level buffer.
      request-entity-chunk-size = 65536 b

      # The number of request data bytes the HTTP/2 implementation is allowed to buffer internally per connection. Free
      # space in this buffer is communicated to the peer using HTTP/2 flow-control messages to backpressure data if it
      # isn't read fast enough.
      #
      # When there is no backpressure, this amount will limit the amount of in-flight data. It might need to be increased
      # for high bandwidth-delay-product connections.
      #
      # There is a relation between the `incoming-connection-level-buffer-size` and the `incoming-stream-level-buffer-size`:
      # If incoming-connection-level-buffer-size < incoming-stream-level-buffer-size * number_of_streams, then
      # head-of-line blocking is possible between different streams on the same connection.
      incoming-connection-level-buffer-size = 10 MB

      # The number of request data bytes the HTTP/2 implementation is allowed to buffer internally per stream. Free space
      # in this buffer is communicated to the peer using HTTP/2 flow-control messages to backpressure data if it isn't
      # read fast enough.
      #
      # When there is no backpressure, this amount will limit the amount of in-flight data per stream. It might need to
      # be increased for high bandwidth-delay-product connections.
      incoming-stream-level-buffer-size = 512kB

      # The maximum number of outgoing control frames to buffer when the peer does not read from its TCP connection before
      # backpressuring incoming frames.
      #
      # On a healthy HTTP/2 connection this setting should have little effect because control frames are given priority over
      # data frames and should not be buffered for a long time.
      #
      # The limit is necessary to prevent a malicious peer to solicit buffering of outgoing control frames (e.g. by sending PINGs)
      # without ever reading frames ultimately leading to an out of memory situation. With the limit in place, the implementation
      # stops reading incoming frames when the number of outgoing control frames has reached the given amount. This way an attacker
      # isn't able to communicate any further without first freeing space in the TCP window, draining the buffered control frames.
      #
      # See CVE-2019-9512 for an example of such an attack.
      #
      # Note that only control frames are affected because data frames, in contrast, are covered by the HTTP/2 flow control.
      outgoing-control-frame-buffer-size = 1024

      # Enable verbose debug logging for all ingoing and outgoing frames
      log-frames = false
    }

    websocket {
      # periodic keep alive may be implemented using by sending Ping frames
      # upon which the other side is expected to reply with a Pong frame,
      # or by sending a Pong frame, which serves as unidirectional heartbeat.
      # Valid values:
      #   ping - default, for bi-directional ping/pong keep-alive heartbeating
      #   pong - for uni-directional pong keep-alive heartbeating
      #
      # It is also possible to provide a payload for each heartbeat message,
      # this setting can be configured programatically by modifying the websocket settings.
      # See: https://doc.akka.io/docs/akka-http/current/server-side/websocket-support.html
      periodic-keep-alive-mode = ping

      # Interval for sending periodic keep-alives
      # The frame sent will be the one configured in akka.http.server.websocket.periodic-keep-alive-mode
      # `infinite` by default, or a duration that is the max idle interval after which an keep-alive frame should be sent
      # The value `infinite` means that *no* keep-alive heartbeat will be sent, as: "the allowed idle time is infinite"
      periodic-keep-alive-max-idle = infinite
    }
  }

  client {
    # The default value of the `User-Agent` header to produce if no
    # explicit `User-Agent`-header was included in a request.
    # If this value is the empty string and no header was included in
    # the request, no `User-Agent` header will be rendered at all.
    user-agent-header = akka-http/${akka.http.version}

    # The time period within which the TCP connecting process must be completed.
    connecting-timeout = 10s

    # The time after which an idle connection will be automatically closed.
    # Set to `infinite` to completely disable idle timeouts.
    idle-timeout = 60 s

    # The initial size of the buffer to render the request headers in.
    # Can be used for fine-tuning request rendering performance but probably
    # doesn't have to be fiddled with in most applications.
    request-header-size-hint = 512

    # Socket options to set for the listening socket. If a setting is left
    # undefined, it will use whatever the default on the system is.
    socket-options {
      so-receive-buffer-size = undefined
      so-send-buffer-size = undefined
      so-reuse-address = undefined
      so-traffic-class = undefined
      tcp-keep-alive = undefined
      tcp-oob-inline = undefined
      tcp-no-delay = undefined
    }

    # Client https proxy options. When using ClientTransport.httpsProxy() with or without credentials,
    # host/port must be either passed explicitly or set here. If a host is not set, the proxy will not be used.
    proxy {
      https {
        host = ""
        port = 443
      }
    }

    # Modify to tweak parsing settings on the client-side only.
    parsing {
      # no overrides by default, see `akka.http.parsing` for default values
    }

    # Enables/disables the logging of unencrypted HTTP traffic to and from the HTTP
    # client for debugging reasons.
    #
    # Note: Use with care. Logging of unencrypted data traffic may expose secret data.
    #
    # Incoming and outgoing traffic will be logged in hexdump format. To enable logging,
    # specify the number of bytes to log per chunk of data (the actual chunking depends
    # on implementation details and networking conditions and should be treated as
    # arbitrary).
    #
    # For logging on the server side, see akka.http.server.log-unencrypted-network-bytes.
    #
    # `off` : no log messages are produced
    # Int   : determines how many bytes should be logged per data chunk
    log-unencrypted-network-bytes = off

    websocket {
      # periodic keep alive may be implemented using by sending Ping frames
      # upon which the other side is expected to reply with a Pong frame,
      # or by sending a Pong frame, which serves as unidirectional heartbeat.
      # Valid values:
      #   ping - default, for bi-directional ping/pong keep-alive heartbeating
      #   pong - for uni-directional pong keep-alive heartbeating
      #
      # See https://tools.ietf.org/html/rfc6455#section-5.5.2
      # and https://tools.ietf.org/html/rfc6455#section-5.5.3 for more information
      periodic-keep-alive-mode = ping

      # Interval for sending periodic keep-alives
      # The frame sent will be the onne configured in akka.http.server.websocket.periodic-keep-alive-mode
      # `infinite` by default, or a duration that is the max idle interval after which an keep-alive frame should be sent
      periodic-keep-alive-max-idle = infinite
    }

    # Cancellation in the HTTP streams is delayed by this duration to prevent race conditions between cancellation
    # and stream completion / failure. In most cases, the value chosen here should make no difference because
    # HTTP streams are loops where completion and failures should propagate immediately and make the handling of
    # cancellations redundant.
    #
    # In most cases, there should be no reason to change this setting.
    #
    # Set to 0 to disable the delay.
    stream-cancellation-delay = 100 millis
  }

  host-connection-pool {
    # The maximum number of parallel connections that a connection pool to a
    # single host endpoint is allowed to establish. Must be greater than zero.
    max-connections = 4

    # The minimum number of parallel connections that a pool should keep alive ("hot").
    # If the number of connections is falling below the given threshold, new ones are being spawned.
    # You can use this setting to build a hot pool of "always on" connections.
    # Default is 0, meaning there might be no active connection at given moment.
    # Keep in mind that `min-connections` should be smaller than `max-connections` or equal
    min-connections = 0

    # The maximum number of times failed requests are attempted again,
    # (if the request can be safely retried) before giving up and returning an error.
    # Set to zero to completely disable request retries.
    max-retries = 5

    # The maximum number of open requests accepted into the pool across all
    # materializations of any of its client flows.
    # Protects against (accidentally) overloading a single pool with too many client flow materializations.
    # Note that with N concurrent materializations the max number of open request in the pool
    # will never exceed N * max-connections * pipelining-limit.
    # Must be a power of 2 and > 0!
    max-open-requests = 32

    # The maximum duration for a connection to be kept alive
    # This amount gets modified by a 10 percent fuzzyness to avoid the simultanous reconnections
    # defaults to 'infinite'
    # Note that this is only implemented in the new host connection pool
    max-connection-lifetime = infinite

    # Client-side pipelining is not currently supported. See https://github.com/akka/akka-http/issues/32
    pipelining-limit = 1

    # The minimum duration to backoff new connection attempts after the previous connection attempt failed.
    #
    # The pool uses an exponential randomized backoff scheme. After the first failure, the next attempt will only be
    # tried after a random duration between the base connection backoff and twice the base connection backoff. If that
    # attempt fails as well, the next attempt will be delayed by twice that amount. The total delay is capped using the
    # `max-connection-backoff` setting.
    #
    # The backoff applies for the complete pool. I.e. after one failed connection attempt, further connection attempts
    # to that host will backoff for all connections of the pool. After the service recovered, connections will come out
    # of backoff one by one due to the random extra backoff time. This is to avoid overloading just recently recovered
    # services with new connections ("thundering herd").
    #
    # Example: base-connection-backoff = 100ms, max-connection-backoff = 10 seconds
    #   - After 1st failure, backoff somewhere between 100ms and 200ms
    #   - After 2nd, between  200ms and  400ms
    #   - After 3rd, between  200ms and  400ms
    #   - After 4th, between  400ms and  800ms
    #   - After 5th, between  800ms and 1600ms
    #   - After 6th, between 1600ms and 3200ms
    #   - After 7th, between 3200ms and 6400ms
    #   - After 8th, between 5000ms and 10 seconds (max capped by max-connection-backoff, min by half of that)
    #   - After 9th, etc., stays between 5000ms and 10 seconds
    #
    # This setting only applies to the new pool implementation and is ignored for the legacy one.
    base-connection-backoff = 100ms

    # Maximum backoff duration between failed connection attempts. For more information see the above comment for the
    # `base-connection-backoff` setting.
    #
    # This setting only applies to the new pool implementation and is ignored for the legacy one.
    max-connection-backoff = 2 min

    # The time after which an idle connection pool (without pending requests)
    # will automatically terminate itself. Set to `infinite` to completely disable idle timeouts.
    idle-timeout = 30 s

    # The pool implementation to use. Currently supported are:
    #  - legacy: the original 10.0.x pool implementation
    #  - new: the pool implementation that became the default in 10.1.x and will receive fixes and new features
    pool-implementation = new

    # The "new" pool implementation will fail a connection early and clear the slot if a response entity was not
    # subscribed during the given time period after the response was dispatched. In busy systems the timeout might be
    # too tight if a response is not picked up quick enough after it was dispatched by the pool.
    response-entity-subscription-timeout = 1.second

    # Modify this section to tweak client settings only for host connection pools APIs like `Http().superPool` or
    # `Http().singleRequest`.
    client = {
      # no overrides by default, see `akka.http.client` for default values
    }
  }

  # Modify to tweak default parsing settings.
  #
  # IMPORTANT:
  # Please note that this sections settings can be overridden by the corresponding settings in:
  # `akka.http.server.parsing`, `akka.http.client.parsing` or `akka.http.host-connection-pool.client.parsing`.
  parsing {
    # The limits for the various parts of the HTTP message parser.
    max-uri-length             = 2k
    max-method-length          = 16
    max-response-reason-length = 64
    max-header-name-length     = 64
    max-header-value-length    = 8k
    max-header-count           = 64
    max-chunk-ext-length       = 256
    max-chunk-size             = 1m

    # Default maximum content length which should not be exceeded by incoming request entities.
    # Can be changed at runtime (to a higher or lower value) via the `HttpEntity::withSizeLimit` method.
    # Note that it is not necessarily a problem to set this to a high value as all stream operations
    # are always properly backpressured.
    # Nevertheless you might want to apply some limit in order to prevent a single client from consuming
    # an excessive amount of server resources.
    #
    # Set to `infinite` to completely disable entity length checks. (Even then you can still apply one
    # programmatically via `withSizeLimit`.)
    max-content-length = 8m

    # The maximum number of bytes to allow when reading the entire entity into memory with `toStrict`
    # (which is used by the `toStrictEntity` and `extractStrictEntity` directives)
    max-to-strict-bytes = 8m

    # Sets the strictness mode for parsing request target URIs.
    # The following values are defined:
    #
    # `strict`: RFC3986-compliant URIs are required,
    #     a 400 response is triggered on violations
    #
    # `relaxed`: all visible 7-Bit ASCII chars are allowed
    #
    uri-parsing-mode = strict

    # Sets the parsing mode for parsing cookies.
    # The following value are defined:
    #
    # `rfc6265`: Only RFC6265-compliant cookies are parsed. Surrounding double-quotes are accepted and
    #   automatically removed. Non-compliant cookies are silently discarded.
    # `raw`: Raw parsing allows any non-control character but ';' to appear in a cookie value. There's no further
    #   post-processing applied, so that the resulting value string may contain any number of whitespace, unicode,
    #   double quotes, or '=' characters at any position.
    #   The rules for parsing the cookie name are the same ones from RFC 6265.
    #
    cookie-parsing-mode = rfc6265

    # Enables/disables the logging of warning messages in case an incoming
    # message (request or response) contains an HTTP header which cannot be
    # parsed into its high-level model class due to incompatible syntax.
    # Note that, independently of this settings, akka-http will accept messages
    # with such headers as long as the message as a whole would still be legal
    # under the HTTP specification even without this header.
    # If a header cannot be parsed into a high-level model instance it will be
    # provided as a `RawHeader`.
    # If logging is enabled it is performed with the configured
    # `error-logging-verbosity`.
    illegal-header-warnings = on

    # Sets the list of headers for which illegal values will *not* cause warning logs to be emitted;
    #
    # Adding a header name to this setting list disables the logging of warning messages in case an incoming　message
    # contains an HTTP header which cannot be　parsed into its high-level model class due to incompatible syntax.
    ignore-illegal-header-for = []

    # Parse headers into typed model classes in the Akka Http core layer.
    #
    # If set to `off`, only essential headers will be parsed into their model classes. All other ones will be provided
    # as instances of `RawHeader`. Currently, `Connection`, `Host`, and `Expect` headers will still be provided in their
    # typed model. The full list of headers still provided as modeled instances can be found in the source code of
    # `akka.http.impl.engine.parsing.HttpHeaderParser.alwaysParsedHeaders`. Note that (regardless of this setting)
    # some headers like `Content-Type` are treated specially and will never be provided in the list of headers.
    modeled-header-parsing = on

    # Configures the verbosity with which message (request or response) parsing
    # errors are written to the application log.
    #
    # Supported settings:
    # `off`   : no log messages are produced
    # `simple`: a condensed single-line message is logged
    # `full`  : the full error details (potentially spanning several lines) are logged
    error-logging-verbosity = full

    # Configures the processing mode when encountering illegal characters in
    # header value of response.
    #
    # Supported mode:
    # `error`  : default mode, throw an ParsingException and terminate the processing
    # `warn`   : ignore the illegal characters in response header value and log a warning message
    # `ignore` : just ignore the illegal characters in response header value
    illegal-response-header-value-processing-mode = error

    # limits for the number of different values per header type that the
    # header cache will hold
    header-cache {
      default = 12
      Content-MD5 = 0
      Date = 0
      If-Match = 0
      If-Modified-Since = 0
      If-None-Match = 0
      If-Range = 0
      If-Unmodified-Since = 0
      User-Agent = 32
    }

    # Enables/disables inclusion of an Tls-Session-Info header in parsed
    # messages over Tls transports (i.e., HttpRequest on server side and
    # HttpResponse on client side).
    tls-session-info-header = off
  }
}
# Copyright (C) Lightbend Inc. <https://www.lightbend.com>

# Configuration for Play's AkkaHttpServer
play {

  server {
    # The server provider class name
    provider = "play.core.server.AkkaHttpServerProvider"

    akka {
      # How long to wait when binding to the listening socket
      bindTimeout = 5 seconds

      # How long a request takes until it times out. Set to null or "infinite" to disable the timeout.
      requestTimeout = infinite

      # Timeout after which all requests and connections shall be forcefully terminated
      # when shutting down the server. It will default to Coordinated Shutdown service-unbind
      # phase timeout. Value must be a duration, for example:
      # play.server.akka.terminationTimeout = 10 seconds
      terminationTimeout = null

      # Enables/disables automatic handling of HEAD requests.
      # If this setting is enabled the server dispatches HEAD requests as GET
      # requests to the application and automatically strips off all message
      # bodies from outgoing responses.
      # Note that, even when this setting is off the server will never send
      # out message bodies on responses to HEAD requests.
      transparent-head-requests = off

      # If this setting is empty the server only accepts requests that carry a
      # non-empty `Host` header. Otherwise it responds with `400 Bad Request`.
      # Set to a non-empty value to be used in lieu of a missing or empty `Host`
      # header to make the server accept such requests.
      # Note that the server will never accept HTTP/1.1 request without a `Host`
      # header, i.e. this setting only affects HTTP/1.1 requests with an empty
      # `Host` header as well as HTTP/1.0 requests.
      # Examples: `www.spray.io` or `example.com:8080`
      default-host-header = ""

      # The default value of the `Server` header to produce if no
      # explicit `Server`-header was included in a response.
      # If this value is null and no header was included in
      # the request, no `Server` header will be rendered at all.
      server-header = null
      server-header = ${?play.server.server-header}

      # Configures the processing mode when encountering illegal characters in
      # header value of response.
      #
      # Supported mode:
      # `error`  : default mode, throw an ParsingException and terminate the processing
      # `warn`   : ignore the illegal characters in response header value and log a warning message
      # `ignore` : just ignore the illegal characters in response header value
      illegal-response-header-value-processing-mode = warn

      # Enables/disables inclusion of an Tls-Session-Info header in parsed
      # messages over Tls transports (i.e., HttpRequest on server side and
      # HttpResponse on client side).
      #
      # See Akka HTTP `akka.http.server.parsing.tls-session-info-header` for
      # more information about how this works.
      tls-session-info-header = on

    }
  }

}
# Copyright (C) Lightbend Inc. <https://www.lightbend.com>

play {
  modules {
    enabled += "play.data.FormFactoryModule"
    enabled += "play.data.format.FormattersModule"
    enabled += "play.data.validation.ValidatorsModule"
  }

  forms {

    binding {

      # Enables or disables direct field access during form binding.
      # If disabled (the default) getter methods will be used to access the form during binding.
      directFieldAccess = false

    }

  }

}
##########################################
# Akka Serialization Jackson Config File #
##########################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.

#//#jackson-modules
akka.serialization.jackson {

  # The Jackson JSON serializer will register these modules.
  jackson-modules += "akka.serialization.jackson.AkkaJacksonModule"
  # AkkaTypedJacksonModule optionally included if akka-actor-typed is in classpath
  jackson-modules += "akka.serialization.jackson.AkkaTypedJacksonModule"
  // FIXME how does that optional loading work??
  # AkkaStreamsModule optionally included if akka-streams is in classpath
  jackson-modules += "akka.serialization.jackson.AkkaStreamJacksonModule"
  jackson-modules += "com.fasterxml.jackson.module.paramnames.ParameterNamesModule"
  jackson-modules += "com.fasterxml.jackson.datatype.jdk8.Jdk8Module"
  jackson-modules += "com.fasterxml.jackson.datatype.jsr310.JavaTimeModule"
  jackson-modules += "com.fasterxml.jackson.module.scala.DefaultScalaModule"
}
#//#jackson-modules

akka.serialization.jackson {
  # When enabled and akka.loglevel=DEBUG serialization time and payload size
  # is logged for each messages.
  verbose-debug-logging = off

  # Define data migration transformations of old formats to current
  # format here as a mapping between the (old) class name to be
  # transformed to the JacksonJsonMigration class that implements
  # the transformation.
  migrations {
  }

}

#//#features
akka.serialization.jackson {
  # Configuration of the ObjectMapper serialization features.
  # See com.fasterxml.jackson.databind.SerializationFeature
  # Enum values corresponding to the SerializationFeature and their boolean value.
  serialization-features {
    # Date/time in ISO-8601 (rfc3339) yyyy-MM-dd'T'HH:mm:ss.SSSZ format
    # as defined by com.fasterxml.jackson.databind.util.StdDateFormat
    # For interoperability it's better to use the ISO format, i.e. WRITE_DATES_AS_TIMESTAMPS=off,
    # but WRITE_DATES_AS_TIMESTAMPS=on has better performance.
    WRITE_DATES_AS_TIMESTAMPS = off
    WRITE_DURATIONS_AS_TIMESTAMPS = off
  }

  # Configuration of the ObjectMapper deserialization features.
  # See com.fasterxml.jackson.databind.DeserializationFeature
  # Enum values corresponding to the DeserializationFeature and their boolean value.
  deserialization-features {
    FAIL_ON_UNKNOWN_PROPERTIES = off
  }

  # Configuration of the ObjectMapper mapper features.
  # See com.fasterxml.jackson.databind.MapperFeature
  # Enum values corresponding to the MapperFeature and their
  # boolean values, for example:
  #
  # mapper-features {
  #   SORT_PROPERTIES_ALPHABETICALLY = on
  # }
  mapper-features {}

  # Configuration of the ObjectMapper JsonParser features.
  # See com.fasterxml.jackson.core.JsonParser.Feature
  # Enum values corresponding to the JsonParser.Feature and their
  # boolean value, for example:
  #
  # json-parser-features {
  #   ALLOW_SINGLE_QUOTES = on
  # }
  json-parser-features {}

  # Configuration of the ObjectMapper JsonParser features.
  # See com.fasterxml.jackson.core.JsonGenerator.Feature
  # Enum values corresponding to the JsonGenerator.Feature and
  # their boolean value, for example:
  #
  # json-generator-features {
  #   WRITE_NUMBERS_AS_STRINGS = on
  # }
  json-generator-features {}

  # Configuration of the JsonFactory StreamReadFeature.
  # See com.fasterxml.jackson.core.StreamReadFeature
  # Enum values corresponding to the StreamReadFeatures and
  # their boolean value, for example:
  #
  # stream-read-features {
  #   STRICT_DUPLICATE_DETECTION = on
  # }
  stream-read-features {}

  # Configuration of the JsonFactory StreamWriteFeature.
  # See com.fasterxml.jackson.core.StreamWriteFeature
  # Enum values corresponding to the StreamWriteFeatures and
  # their boolean value, for example:
  #
  # stream-write-features {
  #   WRITE_BIGDECIMAL_AS_PLAIN = on
  # }
  stream-write-features {}

  # Configuration of the JsonFactory JsonReadFeature.
  # See com.fasterxml.jackson.core.json.JsonReadFeature
  # Enum values corresponding to the JsonReadFeatures and
  # their boolean value, for example:
  #
  # json-read-features {
  #   ALLOW_SINGLE_QUOTES = on
  # }
  json-read-features {}

  # Configuration of the JsonFactory JsonWriteFeature.
  # See com.fasterxml.jackson.core.json.JsonWriteFeature
  # Enum values corresponding to the JsonWriteFeatures and
  # their boolean value, for example:
  #
  # json-write-features {
  #   WRITE_NUMBERS_AS_STRINGS = on
  # }
  json-write-features {}

  # Deprecated, use `allowed-class-prefix` instead
  whitelist-class-prefix = []

  # Additional classes that are allowed even if they are not defined in `serialization-bindings`.
  # This is useful when a class is not used for serialization any more and therefore removed
  # from `serialization-bindings`, but should still be possible to deserialize.
  allowed-class-prefix = ${akka.serialization.jackson.whitelist-class-prefix}


  # settings for compression of the payload
  compression {
    # Compression algorithm.
    # - off  : no compression
    # - gzip : using common java gzip
    algorithm = off

    # If compression is enabled with the `algorithm` setting the payload is compressed
    # when it's larger than this value.
    compress-larger-than = 0 KiB
  }

  # Whether the type should be written to the manifest.
  # If this is off, then either deserialization-type must be defined, or there must be exactly
  # one serialization binding declared for this serializer, and the type in that binding will be
  # used as the deserialization type. This feature will only work if that type either is a
  # concrete class, or if it is a supertype that uses Jackson polymorphism (ie, the
  # @JsonTypeInfo annotation) to store type information in the JSON itself. The intention behind
  # disabling this is to remove extraneous type information (ie, fully qualified class names) when
  # serialized objects are persisted in Akka persistence or replicated using Akka distributed
  # data. Note that Akka remoting already has manifest compression optimizations that address this,
  # so for types that just get sent over remoting, this offers no optimization.
  type-in-manifest = on

  # The type to use for deserialization.
  # This is only used if type-in-manifest is disabled. If set, this type will be used to
  # deserialize all messages. This is useful if the binding configuration you want to use when
  # disabling type in manifest cannot be expressed as a single type. Examples of when you might
  # use this include when changing serializers, so you don't want this serializer used for
  # serialization and you haven't declared any bindings for it, but you still want to be able to
  # deserialize messages that were serialized with this serializer, as well as situations where
  # you only want some sub types of a given Jackson polymorphic type to be serialized using this
  # serializer.
  deserialization-type = ""

  # Specific settings for jackson-json binding can be defined in this section to
  # override the settings in 'akka.serialization.jackson'
  jackson-json {}

  # Specific settings for jackson-cbor binding can be defined in this section to
  # override the settings in 'akka.serialization.jackson'
  jackson-cbor {}

  # Issue #28918 for compatibility with data serialized with JacksonCborSerializer in
  # Akka 2.6.4 or earlier, which was plain JSON format.
  jackson-cbor-264 = ${akka.serialization.jackson.jackson-cbor}

}
#//#features

#//#compression
# Compression settings for the jackson-json binding
akka.serialization.jackson.jackson-json.compression {
  # Compression algorithm.
  # - off  : no compression
  # - gzip : using common java gzip
  # - lz4 : using lz4-java
  algorithm = gzip

  # If compression is enabled with the `algorithm` setting the payload is compressed
  # when it's larger than this value.
  compress-larger-than = 32 KiB
}
#//#compression

akka.actor {
  serializers {
    jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
    jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"

    # Issue #28918 for compatibility with data serialized with JacksonCborSerializer in
    # Akka 2.6.4 or earlier, which was plain JSON format.
    jackson-cbor-264 = "akka.serialization.jackson.JacksonJsonSerializer"
  }
  serialization-identifiers {
    jackson-json = 31
    jackson-cbor = 33

    # Issue #28918 for compatibility with data serialized with JacksonCborSerializer in
    # Akka 2.6.4 or earlier, which was plain JSON format.
    jackson-cbor-264 = 32
  }
  serialization-bindings {
    # Define bindings for classes or interfaces use Jackson serializer, e.g.
    # "com.example.Jsonable" = jackson-json
    # "com.example.MyMessage" = jackson-cbor
    #
    # For security reasons it is disallowed to bind the Jackson serializers to
    # open ended types that might be target to be deserialization gadgets, such as
    # java.lang.Object, java.io.Serializable, java.util.Comparable

  }
}
# Copyright (C) 2015 - 2020 Lightbend Inc. <https://www.lightbend.com>

# ssl configuration
ssl-config {

  logger = "com.typesafe.sslconfig.util.NoopLogger"

  # Whether we should use the default JVM SSL configuration or not
  # When false additional configuration will be applied on the context (as configured in ssl-config).
  default = false

  # The ssl protocol to use
  protocol = "TLSv1.2"

  # Whether revocation lists should be checked, if null, defaults to platform default setting.
  checkRevocation = null

  # A sequence of URLs for obtaining revocation lists
  revocationLists = []

  # The enabled cipher suites. If empty, uses the platform default.
  enabledCipherSuites = []

  # The enabled protocols. If empty, uses the platform default.
  enabledProtocols = ["TLSv1.2", "TLSv1.1", "TLSv1"]

  # The disabled signature algorithms
  disabledSignatureAlgorithms = ["MD2", "MD4", "MD5"]

  # The disabled key algorithms
  disabledKeyAlgorithms = ["RSA keySize < 2048", "DSA keySize < 2048", "EC keySize < 224"]

  # The hostname verifier class.
  # If non null, should be the fully qualify classname of a class that imlpements HostnameVerifier,
  # otherwise the default will be used
  hostnameVerifierClass = null

  sslParameters {
    # translates to a setNeedClientAuth / setWantClientAuth calls
    # "default" – leaves the (which for JDK8 means wantClientAuth and needClientAuth are set to false.)
    # "none"    – `setNeedClientAuth(false)`
    # "want"    – `setWantClientAuth(true)`
    # "need"    – `setNeedClientAuth(true)`
    clientAuth = "default"

    # protocols (names)
    protocols = []
  }

  # Configuration for the key manager
  keyManager {
    # The key manager algorithm. If empty, uses the platform default.
    algorithm = null

    # The key stores
    stores = [
    ]
    # The key stores should look like this
    prototype.stores {
      # The store type. If null, defaults to the platform default store type, ie JKS.
      type = null

      # The path to the keystore file. Either this must be non null, or data must be non null.
      path = null

      # The data for the keystore. Either this must be non null, or path must be non null.
      data = null

      # The password for loading the keystore. If null, uses no password.
      # It's recommended to load password using environment variable
      password = null
    }
  }

  trustManager {
    # The trust manager algorithm. If empty, uses the platform default.
    algorithm = null

    # The trust stores
    stores = [
    ]
    # The key stores should look like this
    prototype.stores {
      # The store type. If null, defaults to the platform default store type, ie JKS.
      type = null

      # The path to the keystore file. Either this must be non null, or data must be non null.
      path = null

      # The data for the keystore. Either this must be non null, or path must be non null.
      data = null

      # The password for loading the truststore. If null, uses no password.
      # It's recommended to load password using environment variable
      password = null
    }

  }

  # The loose ssl options.  These allow configuring ssl to be more loose about what it accepts,
  # at the cost of introducing potential security issues.
  loose {

    # Whether weak protocols should be allowed
    allowWeakProtocols = false

    # Whether weak ciphers should be allowed
    allowWeakCiphers = false

    # If non null, overrides the platform default for whether legacy hello messsages should be allowed.
    allowLegacyHelloMessages = null

    # If non null, overrides the platform defalut for whether unsafe renegotiation should be allowed.
    allowUnsafeRenegotiation = null

    # Whether hostname verification should be disabled
    disableHostnameVerification = false

    # Whether the SNI (Server Name Indication) TLS extension should be disabled
    # This setting MAY be respected by client libraries.
    #
    # https://tools.ietf.org/html/rfc3546#sectiom-3.1
    disableSNI = false

    # Whether any certificate should be accepted or not
    acceptAnyCertificate = false
  }

  # Debug configuration
  debug {
    # Enable all debugging
    all = false

    # Enable sslengine / socket tracing
    ssl = false

    # Enable SSLContext tracing
    sslctx = false

    # Enable key manager tracing
    keymanager = false

    # Enable trust manager tracing
    trustmanager = false

    // The following settings are deprecated and have no effect in code.
    certpath = false # DEPRECATED
    ocsp = false # DEPRECATED
    record = false # DEPRECATED
    plaintext = false # DEPRECATED
    packet = false # DEPRECATED
    handshake = false # DEPRECATED
    data = false # DEPRECATED
    verbose = false # DEPRECATED
    keygen = false # DEPRECATED
    session = false # DEPRECATED
    defaultctx = false # DEPRECATED
    sessioncache = false # DEPRECATED
    pluggability = false # DEPRECATED
  }
}
# Copyright (C) Lightbend Inc. <https://www.lightbend.com>

play {

  server {

    # The root directory for the Play server instance. This value can
    # be set by providing a path as the first argument to the Play server
    # launcher script. See `ServerConfig.loadConfiguration`.
    dir = ${?user.dir}

    # HTTP configuration
    http {
      # The HTTP port of the server. Use a value of "disabled" if the server
      # shouldn't bind an HTTP port.
      port = 9000
      port = ${?PLAY_HTTP_PORT}
      port = ${?http.port}

      # The interface address to bind to.
      address = "0.0.0.0"
      address = ${?PLAY_HTTP_ADDRESS}
      address = ${?http.address}

      # The idle timeout for an open connection after which it will be closed
      # Set to null or "infinite" to disable the timeout, but notice that this
      # is not encouraged since timeout are important mechanisms to protect your
      # servers from malicious attacks or programming mistakes.
      idleTimeout = 75 seconds
    }

    # HTTPS configuration
    https {

      # The HTTPS port of the server.
      port = ${?PLAY_HTTPS_PORT}
      port = ${?https.port}

      # The interface address to bind to
      address = "0.0.0.0"
      address = ${?PLAY_HTTPS_ADDRESS}
      address = ${?https.address}

      # The idle timeout for an open connection after which it will be closed
      # Set to null or "infinite" to disable the timeout, but notice that this
      # is not encouraged since timeout are important mechanisms to protect your
      # servers from malicious attacks or programming mistakes.
      idleTimeout = ${play.server.http.idleTimeout}

      # The SSL engine provider
      engineProvider = "play.core.server.ssl.DefaultSSLEngineProvider"
      engineProvider = ${?play.http.sslengineprovider}

      # HTTPS keystore configuration, used by the default SSL engine provider
      keyStore {
        # The path to the keystore
        path = ${?https.keyStore}

        # The type of the keystore
        type = "JKS"
        type = ${?https.keyStoreType}

        # The password for the keystore
        password = ""
        password = ${?https.keyStorePassword}

        # The algorithm to use. If not set, uses the platform default algorithm.
        algorithm = ${?https.keyStoreAlgorithm}
      }

      # HTTPS truststore configuration
      trustStore {

        # If true, does not do CA verification on client side certificates
        noCaVerification = false
      }

      # Whether JSSE want client auth mode should be used. This means, the server
      # will request a client certificate, but won't fail if one isn't provided.
      wantClientAuth = false

      # Whether JSSE need client auth mode should be used. This means, the server
      # will request a client certificate, and will fail and terminate the session
      # if one isn't provided.
      needClientAuth = false
    }

    # The path to the process id file created by the server when it runs.
    # If set to "/dev/null" then no pid file will be created.
    pidfile.path = ${play.server.dir}/RUNNING_PID
    pidfile.path = ${?pidfile.path}

    websocket {
      # Maximum allowable frame payload length. Setting this value to your application's
      # requirement may reduce denial of service attacks using long data frames.
      frame.maxLength = 64k
      frame.maxLength = ${?websocket.frame.maxLength}
    }

    debug {
      # If set to true this will attach an attribute to each request containing debug information. If the application
      # fails to load (e.g. due to a compile issue in dev mode), then this configuration value is ignored and the debug
      # information is always attached.
      #
      # Note: This configuration option is not part of Play's public API and is subject to change without the usual
      # deprecation cycle.
      addDebugInfoToRequests = false
    }

    # The maximum length of the HTTP headers. The most common effect of this is a restriction in cookie length, including
    # number of cookies and size of cookie values.
    max-header-size = 8k

    # If a request contains a Content-Length header it will be checked against this maximum value.
    # If the value of a given Content-Length header exceeds this configured value, the request will not be processed
    # further but instead the error handler will be called with Http status code 413 "Entity too large".
    # If set to infinite or if no Content-Length header exists then no check will take place at all
    # and the request will continue to be processed.
    # Play uses the concept of a `BodyParser` to enforce this limit, so we set it to infinite.
    max-content-length = infinite
  }

  editor = ${?PLAY_EDITOR}

}
